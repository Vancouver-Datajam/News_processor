# -*- coding: utf-8 -*-
"""sentimental_Positive_Rewrite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zqE9aMlYWdj1fsBTjc4jQL1gz4PsvugY
"""

import tensorflow as tf
import requests
import zipfile
import requests
import os
import time
import random
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow.keras.layers as layers
import tensorflow.keras.models as models
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.callbacks import EarlyStopping, CSVLogger
import numpy as np
from PIL import Image
import pickle
from tensorflow.keras.models import load_model, Model
from PIL import Image
from PIL.PngImagePlugin import PngImageFile
import matplotlib.pyplot as plt
import glob
from functools import partial
import nltk
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS


# Seeding
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except:
        print("Couldn't set memory_growth")
        pass


def fix_random_seed(seed):
    """ Setting the random seed of various libraries """
    try:
        np.random.seed(seed)
    except NameError:
        print("Warning: Numpy is not imported. Setting the seed for Numpy failed.")
    try:
        tf.random.set_seed(seed)
    except NameError:
        print("Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.")
    try:
        random.seed(seed)
    except NameError:
        print("Warning: random module is not imported. Setting the seed for random failed.")

# Fixing the random seed
random_seed=4321
fix_random_seed(random_seed)

print("TensorFlow version: {}".format(tf.__version__))

"""**Data Preparation/Preprocessing**"""

import pandas as pd
import requests
import pyarrow.parquet as pq


test_file = 'data/test.parquet'
train_file = 'data/train.parquet'

test_table = pq.read_table(test_file)
train_table = pq.read_table(train_file)

train_df = train_table.to_pandas()
test_df = test_table.to_pandas()
dataframe = pd.concat([train_df,test_df])
dataframe.columns = ['message', 'sentiment']
print(dataframe.head())
print(dataframe.keys())

'''
Cleaning up data - removing null or empty data.
Will need to change the column header depending on the dataset ("message")
'''
print("Before cleaning up: {}".format(dataframe.shape))
dataframe = dataframe[~dataframe["message"].isna()]
dataframe = dataframe[dataframe["message"].str.strip().str.len()>0]
print("After cleaning up: {}".format(dataframe.shape))

'''
Depending on what we want, we can decide on how we want to map
the sentiments to classses for training

For now, we can work off the assumption of the following
1 and 2 -> 1
0 and -1 -> 0

The final counts are printed out below
Skip this if working with parquet data
'''
# dataframe["label"] = dataframe["sentiment"].map({1: 1, 2: 1, 0: 0, -1: 0})
dataframe["label"] = dataframe["sentiment"].map({1: 1, 2: 2, 0: 0})
print(dataframe.head())
print("\n")
# Number of each sentiment in the dataset
print( dataframe["label"].value_counts())

# shuffle data
dataframe = dataframe.sample(frac=1.0, random_state=random_seed)

# split data into inputs and targets
inputs, labels = dataframe["message"], dataframe["label"]
print(labels.shape)

'''
Performing the following
Lower case (nltk) - Turn "I am" to "i am"
Remove numbers (regex) - Turn "i am 24 years old" to "i am years old"
Remove stop words (nltk) - Turn "i go to the shop" to "i go shop"
Lemmatize (nltk) - Turn "i went to buy flowers" to "i go to buy flower"
'''

import nltk
nltk.download('averaged_perceptron_tagger', download_dir='nltk')
nltk.download('wordnet', download_dir='nltk')
nltk.download('stopwords', download_dir='nltk')
nltk.download('punkt', download_dir='nltk')
nltk.download('omw-1.4', download_dir='nltk')
nltk.data.path.append(os.path.abspath('nltk'))

from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import string
from collections import Counter
from nltk.probability import FreqDist
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import train_test_split, RandomizedSearchCV
rerun = False

# Define a lemmatizer (converts words to base form)
lemmatizer = WordNetLemmatizer()

# Define the English stopwords
EN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}

# Code listing 9.2
def clean_text(doc):
    """ A function that cleans a given document (i.e. a text string)"""

    # Turn to lower case
    doc = doc.lower()
    # the shortened form n't is expanded to not
    doc = re.sub(pattern=r"\w+n\'t ", repl="not ", string=doc)
    # shortened forms like 'll 're 'd 've are removed as they don't add much value to this task
    doc = re.sub(r"(?:\'ll |\'re |\'d |\'ve )", " ", doc)
    # numbers are removed
    doc = re.sub(r"/d+","", doc)
    # break the text in to tokens (or words), while doing that ignore stopwords from the result
    # stopwords again do not add any value to the task
    tokens = [w for w in word_tokenize(doc) if w not in EN_STOPWORDS and w not in string.punctuation]

    # Here we lemmatize the words in the tokens
    # to lemmatize, we get the pos tag of each token and
    # if it is N (noun) or V (verb) we lemmatize, else
    # keep the original form
    pos_tags = nltk.pos_tag(tokens)
    clean_text = [
        lemmatizer.lemmatize(w, pos=p[0].lower()) \
        if p[0]=='N' or p[0]=='V' else w \
        for (w, p) in pos_tags
    ]

    # return the clean text
    return clean_text

if rerun or \
    not os.path.exists('sentiment_inputs.pkl') or \
    not os.path.exists('sentiment_labels.pkl'):
    # Apply the transformation to the full text
    # this is time consuming
    print("\nProcessing all the review data ... This can take some time (several minutes)")
    inputs = inputs.apply(lambda x: clean_text(x))
    print("\tDone")

    print("Saving the data")
    inputs.to_pickle('sentiment_inputs.pkl')
    labels.to_pickle('sentiment_labels.pkl')

else:
    # Load the data from the disk
    print("Data already found. If you want to rerun anyway, set rerun=True")
    inputs = pd.read_pickle( 'sentiment_inputs.pkl')
    labels = pd.read_pickle( 'sentiment_labels.pkl')
    # labels = tf.keras.utils.to_categorical(labels, num_classes=3)
    # print(labels[0])

for actual, clean in zip(dataframe["message"].iloc[:5], inputs.iloc[:5]):
  print("Actual: {}".format(actual))
  print("Clean: {}".format(clean))
  print('\n')

"""**Data Vizualization**"""

# change sentiment from int to words for easy read
df = dataframe.copy()
sentiment = df['sentiment']
word_sentiment = []

for i in sentiment :
    if i == 2 :
        word_sentiment.append('Positive')
    elif i == 1 :
        word_sentiment.append('Neutral')
    elif i == 0 :
        word_sentiment.append('Negative')
    else :
            word_sentiment.append('News')


df['sentiment'] = word_sentiment

def frequency(news):
    """
    This function determines the frequency of each word in our dataset
    and stores the most frequent words in a dataframe,
    sorted from most to least frequent
    """

    # Count vectorizer excluding english stopwords
    cv = CountVectorizer(stop_words='english')
    words = cv.fit_transform(news)

    # Count the words in the news and determine the frequency of each word
    sum_words = words.sum(axis=0)
    words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

    # Create a dataframe to store the top 30 words and their frequencies
    frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])
    frequency = frequency.head(30)

    return frequency


# Extract the words
news_frequency = frequency(df['message'])
pro_frequency = frequency(df['message'][df['sentiment']=='Positive'])
anti_frequency = frequency(df['message'][df['sentiment']=='Negative'])
neutral_frequency = frequency(df['message'][df['sentiment']=='Neutral'])

news_words = ' '.join([text for text in news_frequency['word']])
anti_words = ' '.join([text for text in anti_frequency['word']])
pro_words = ' '.join([text for text in pro_frequency['word']])
neutral_words = ' '.join([text for text in neutral_frequency['word']])

# Create wordcloud for the climate change classes
stopwords = set(STOPWORDS)
stopwords_custom = {"year", "including", "related"}
stopwords.update(stopwords_custom)

anti_wordcloud = WordCloud(width=800,
                           height=500,
                           random_state=110,
                           max_font_size=110,
                           background_color='white',
                           stopwords = stopwords,
                           colormap="Reds").generate(anti_words)


pro_wordcloud = WordCloud(width=800,
                          height=500,
                          random_state=73,
                          max_font_size=110,
                          background_color='white',
                          stopwords = stopwords,
                          colormap="Greens").generate(pro_words)


news_wordcloud = WordCloud(width=800,
                          height=500,
                          random_state=0,
                          max_font_size=110,
                          background_color='white',
                          stopwords = stopwords,
                          colormap="Blues").generate(news_words)


neutral_wordcloud = WordCloud(width=800,
                          height=500,
                          random_state=10,
                          max_font_size=110,
                          background_color='white',
                          stopwords = stopwords,
                          colormap="Oranges").generate(neutral_words)


# Plot pro and anti wordclouds next to one another for comparisson
f, axarr = plt.subplots(2,2, figsize=(35,25))
axarr[0,0].imshow(pro_wordcloud, interpolation="bilinear")
axarr[0,1].imshow(anti_wordcloud, interpolation="bilinear")
axarr[1,0].imshow(neutral_wordcloud, interpolation="bilinear")
axarr[1,1].imshow(news_wordcloud, interpolation="bilinear")

# Remove the ticks on the x and y axes
for ax in f.axes:
    plt.sca(ax)
    plt.axis('off')

axarr[0,0].set_title('Positive\n', fontsize=35)
axarr[0,1].set_title('Negative \n', fontsize=35)
axarr[1,0].set_title('Neutral\n', fontsize=35)
axarr[1,1].set_title('News\n', fontsize=35)
#plt.tight_layout()
plt.show()

# Create wordcolud for the news climate change dataframe
news_wordcloud = WordCloud(width=800,
                          height=500,
                          random_state=0,
                          max_font_size=110,
                          background_color='white',
                          stopwords = stopwords,
                          colormap="Blues").generate(news_words)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(news_wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
plt.show()

import spacy
# Downloads
nlp = spacy.load('en_core_web_sm')

def entity_extractor(news):

    """
    This function extracts the top 10 people, organizations and geopolitical entities
    in a collection of news.
    The information is then saved in a new dataframe

    Input:
    news: lemmatized news
           datatype: 'str'

    Output:
    df: dataframe containing the top 10 people, organizations and gpe's in a collection of news
        datatype: dataframe ('str')
    """

    def get_people(news):
        words = nlp(news)
        people = [w.text for w in words.ents if w.label_== 'PERSON']
        return people

    def get_org(news):
        words = nlp(news)
        org = [w.text for w in words.ents if w.label_== 'ORG']
        return org

    def get_gpe(news):
        words = nlp(news)
        gpe = [w.text for w in words.ents if w.label_== 'GPE']
        return gpe

    # Extract the top 10 people
    people = news.apply(lambda x: get_people(x))
    people = [x for sub in people for x in sub]
    people_counter = Counter(people)
    people_count = people_counter.most_common(10)
    people, people_count = map(list, zip(*people_count))

    # Extract the top 10 organizations
    org = news.apply(lambda x: get_org(x))
    org = [x for sub in org for x in sub]
    org_counter = Counter(org)
    org_count = org_counter.most_common(10)
    org, org_count = map(list, zip(*org_count))

    # Extract the top 10 geopolitical entities
    gpe = news.apply(lambda x: get_gpe(x))
    gpe = [x for sub in gpe for x in sub]
    gpe_counter = Counter(gpe)
    gpe_count = gpe_counter.most_common(10)
    gpe, gpe_count = map(list, zip(*gpe_count))

    # Create a dataframe to store the information
    df = pd.DataFrame({'people' : people})
    df['geopolitics'] = gpe
    df['organizations'] = org

    return df

# Extract top entities for each class
anti_info = entity_extractor(df['message'][df['sentiment']=='Negative'])
pro_info = entity_extractor(df['message'][df['sentiment']=='Positive'])

print('Positive climate change information')
display(pro_info.head(9))

print('Negative climate change information')
display(anti_info)

"""**Model Exploration and Training**"""

def train_valid_test_split(inputs, labels, train_fraction=0.8):
    """ Splits a given dataset into three sets; training, validation and test """

    # Separate indices of negative and positive data points
    neg_indices = pd.Series(labels.loc[(labels==0)].index)
    neut_indices = pd.Series(labels.loc[(labels==1)].index)
    pos_indices = pd.Series(labels.loc[(labels==2)].index)

    n_valid = int(min([len(neg_indices), len(neut_indices), len(pos_indices)]) * ((1-train_fraction)/2.0))
    n_test = n_valid

    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)
    neg_valid_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds)].sample(n=n_test, random_state=random_seed)
    neg_train_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds.tolist()+neg_valid_inds.tolist())]

    neut_test_inds = neut_indices.sample(n=n_test, random_state=random_seed)
    neut_valid_inds = neut_indices.loc[~neut_indices.isin(neut_test_inds)].sample(n=n_test, random_state=random_seed)
    neut_train_inds = neut_indices.loc[~neut_indices.isin(neut_test_inds.tolist()+neut_valid_inds.tolist())]

    pos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed)
    pos_valid_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds)].sample(n=n_test, random_state=random_seed)
    pos_train_inds = pos_indices.loc[
        ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())
    ]

    tr_x = inputs.loc[neg_train_inds.tolist() + neut_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)
    tr_y = labels.loc[neg_train_inds.tolist() + neut_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)
    v_x = inputs.loc[neg_valid_inds.tolist() + neut_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)
    v_y = labels.loc[neg_valid_inds.tolist() + neut_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)
    ts_x = inputs.loc[neg_test_inds.tolist() + neut_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)
    ts_y = labels.loc[neg_test_inds.tolist() + neut_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)

    # tr_y = pd.get_dummies(tr_y, columns=['label'], prefix='class')
    # v_y = pd.get_dummies(v_y, columns=['label'], prefix='class')
    # ts_y = pd.get_dummies(ts_y, columns=['label'], prefix='class')

    print('Training data: {}'.format(len(tr_x)))
    print('Validation data: {}'.format(len(v_x)))
    print('Test data: {}'.format(len(ts_x)))

    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)

(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(inputs, labels)

print("Some sample targets")
print(tr_y.head())
print(tr_x.head())

'''
Analysis of vocabulary
'''
from collections import Counter
# Create a large list which contains all the words in all the reviews
data_list = [w for doc in tr_x for w in doc]

# Create a Counter object from that list
# Counter returns a dictionary, where key is a word and the value is the frequency
cnt = Counter(data_list)

# Convert the result to a pd.Series
freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)
# Print most common words
print(freq_df.head(n=10))

# Print summary statistics
print(freq_df.describe())

'''
Analysing thesequence length (number of words) of news
'''
# Create a pd.Series, which contain the sequence length for each review
seq_length_ser = tr_x.str.len()

# Get the median as well as summary statistics of the sequence length
print("\nSome summary statistics")
print("Median length: {}\n".format(seq_length_ser.median()))
seq_length_ser.describe()

print("\nComputing the statistics between the 10% and 90% quantiles (to ignore outliers)")
p_10 = seq_length_ser.quantile(0.1)
p_90 = seq_length_ser.quantile(0.9)

seq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe(percentiles=[0.33, 0.66])

n_vocab = (freq_df >= 10).sum()
print("Using a vocabulary of size: {}".format(n_vocab))

'''
Transforming text to numbers
'''
from tensorflow.keras.preprocessing.text import Tokenizer

# Define a tokenizer that will convert words to IDs
# words that are less frequent will be replaced by 'unk'
tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)

# Fit the tokenizer on the data
tokenizer.fit_on_texts(tr_x.tolist())

# Convert all of train/validation/test data to sequences of IDs
tr_x = tokenizer.texts_to_sequences(tr_x.tolist())
v_x = tokenizer.texts_to_sequences(v_x.tolist())
ts_x = tokenizer.texts_to_sequences(ts_x.tolist())

# Checking the attributes of the tokenizer
word = "climate"
wid = tokenizer.word_index[word]
print("The word id for \"{}\" is: {}".format(word, wid))
wid = 4
word = tokenizer.index_word[wid]
print("The word for id {} is: {}".format(wid, word))

# Convert words to IDs

# Vocabs not used get assigned id of 1 I think, so since they're not being
# used in the training, it doesn't actually matter
test_text = [
    ['environmental', 'management', 'system', 'encompass', 'series', 'metric', 'related', 'greenhouse', 'gas', 'emission', 'capture', 'forest', 'restoration', 'water', 'withdrawal', 'use', 'energy', 'use', 'generation', 'well', 'waste', 'generation', 'disposal'],
    ['target', 'consume', '100', 'renewable', 'electricity', 'july', '2021', 'europe', '2025', 'market'],
    ['january', '2020', 'new', 'estimation', 'method', 'report', 'unaccounted', 'gas', 'uafg', 'emission', 'result', 'natural', 'gas', 'distribution', 'activity', 'introduce', 'australia', 'approach', 'enables', 'site/network', 'specific', 'uafg', 'value', 'use', 'allow', 'us', 'translate', 'network', 'maintenance', 'replacement', 'activity', 'reportable', 'reduction', 'uafg', 'emission'],
    ['future', 'work', 'climate', 'continue', 'high', 'ap2', "'s", 'agenda', 'implementation', 'task', 'force', 'climate-related', 'financial', 'disclosure', 'part', 'fund', "'s", 'on-going', 'work', '2020', 'focus', 'implement', 'new', 'sustainability', 'strategy', 'strong', 'focus', 'climate', 'among', 'thing', 'fund', 'develop', 'internal', 'index', 'sustainability', 'profile', 'ambition', 'comply', 'criterion', 'eu', 'paris', 'align', 'benchmark', 'asset', 'management', 'work', 'actively', 'different', 'way', 'include', 'climate', 'risk', 'opportunity', 'analysis', 'find', 'investment', 'opportunity', 'different', 'asset', 'class', 'integrate', 'climate', 'analysis', 'overall', 'alm', 'analysis', 'continue', 'develo-', 'ped', 'fund', 'also', 'intend', 'develop', 'view', 'significant', 'climate', 'risk', 'opportunity', 'asset', 'classes/sectors/', 'geography', 'time', 'horizon'],
    ['however', 'factor', 'may', 'lead', 'commencement', 'engagement', 'decide', 'particularly', 'focus', 'company', 'relation', 'particular', 'esg-related', 'concern', 'not', 'publish', 'adequate', 'environmental', 'information', '‘', 'laggard', '’', 'regard', 'commitment', 'address', 'climate', 'change', 'issue'],
]

test_seq = tokenizer.texts_to_sequences(test_text)

for text, seq in zip(test_text, test_seq):
    print("Text: {}".format(text))
    print("Sequence: {}".format(seq))
    print("\n")

def get_tf_pipeline(text_seq, labels, batch_size=64, bucket_boundaries=[5,15], max_length=50, shuffle=False):
    """ Define a data pipeline that converts sequences to batches of data """

    num_classes = 3
    # Concatenate the label and the input sequence so that we don't mess up the order when we shuffle
    data_seq = [[b] + a for a, b in zip(text_seq, labels)]
    # Define the variable sequence dataset as a ragged tensor
    tf_data = tf.ragged.constant(data_seq)[:, :max_length]

    # One-hot encode labels
    labels = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)

    # Create a dataset out of the ragged tensor
    text_ds = tf.data.Dataset.from_tensor_slices((tf_data, labels))
    text_ds = text_ds.filter(lambda x, y: tf.size(x) > 1)

    # Bucketing the data
    # Bucketing assigns each sequence to a bucket depending on the length
    # If you define bucket boundaries as [5, 15], then you get buckets,
    # [0, 5], [5, 15], [15,inf]
    bucket_fn = tf.data.experimental.bucket_by_sequence_length(
        lambda x, y: tf.cast(tf.shape(x)[0], 'int32'),
        bucket_boundaries=bucket_boundaries,
        bucket_batch_sizes=[batch_size, batch_size, batch_size],
        padded_shapes=(tf.TensorShape([max_length]), tf.TensorShape([num_classes])),
        padding_values=(0, 0),
        pad_to_bucket_boundary=False
    )

    # Apply bucketing
    text_ds = text_ds.map(lambda x, y: (x, y)).apply(bucket_fn)

    # Shuffle the data
    if shuffle:
        text_ds = text_ds.shuffle(buffer_size=10 * batch_size)

    return text_ds

'''Testing the bucketing function '''
train_ds = get_tf_pipeline(tr_x, tr_y, shuffle=True)
valid_ds = get_tf_pipeline(v_x, v_y)

print("Some training data ...")
for x,y in train_ds.take(2):
    print("Input sequence shape: {}".format(x.shape))
    print(y)

print("\nSome validation data ...")
for x,y in valid_ds.take(2):
    print("Input sequence shape: {}".format(x.shape))
    print(y)

import tensorflow.keras.backend as K
from keras.losses import categorical_crossentropy

K.clear_session()

class OnehotEncoder(tf.keras.layers.Layer):
    def __init__(self, depth, **kwargs):
        super(OnehotEncoder, self).__init__(**kwargs)
        self.depth = depth

    def build(self, input_shape):
        pass

    def call(self, inputs):

        inputs = tf.cast(inputs, 'int32')

        if len(inputs.shape) == 3:
            inputs = inputs[:,:,0]
        return tf.one_hot(inputs, depth=self.depth)


    def compute_mask(self, inputs, mask=None):
        return mask

    def get_config(self):
        config = super().get_config().copy()
        config.update({'depth': self.depth})
        return config

# You will see the following error if you don't filter out all zero (empty) records from the dataset
# these records return a vector of all zeros which leads the LSTM layer to error out
# CUDNN_STATUS_BAD_PARAM
# in tensorflow/stream_executor/cuda/cuda_dnn.cc(1496):
# 'cudnnSetRNNDataDescriptor(
#     data_desc.get(), data_type,
#     layout,
#     max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill
# )'

# Code listing 9.5

model = tf.keras.models.Sequential([
    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=(None,)),
    # Create a mask to mask out zero inputs
    tf.keras.layers.Masking(mask_value=0),
    # After creating the mask, convert inputs to onehot encoded inputs
    OnehotEncoder(depth=n_vocab),
    # Defining an LSTM layer
    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),
    # tf.keras.layers.Flatten(),
    # Defining a Dense layer
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')
])
# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

'''Checking the mask '''
inp = tf.expand_dims(tf.constant([[2,3,4,0,0], [2,4,6,12,0]], dtype='int32'),axis=-1)
# Create a mask to mask out zero inputs
mask_out = tf.keras.layers.Masking(mask_value=0)(inp)
print("Masking layer's mask")
print(mask_out._keras_mask)
# After creating the mask, convert inputs to onehot encoded inputs
onehot_out = OnehotEncoder(depth=10)(mask_out)
print("Onehot encoder layer's mask")
print(onehot_out._keras_mask)
# Defining an LSTM layer
lstm_out = tf.keras.layers.LSTM(24, return_state=False, return_sequences=False)(
    onehot_out, mask=onehot_out._keras_mask
)

print("Defining data pipelines")

# Using a batch size of 128
batch_size = 128
train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)
valid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size)
test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)
print('\tDone...')

# There is a class imbalance in the data therefore we are defining a weight for negative inputs
# neg_weight = (tr_y==1).sum()/(tr_y==0).sum()
# print("Will be using a weight of {} for negative samples".format(neg_weight))

from sklearn.utils.class_weight import compute_class_weight

# Assuming you have three classes: 0, 1, and 2
classes = [0,1,2]

# Compute class weights
class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=tr_y)
class_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}
print("Class Weights:", class_weight_dict)

# Section 9.5

os.makedirs('eval', exist_ok=True)

# Logging the performance metrics to a CSV file
csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','1_sentiment_analysis.log'))

monitor_metric = 'val_loss'
mode = 'min'
print("Using metric={} and mode={} for EarlyStopping".format(monitor_metric, mode))

# Reduce LR callback
lr_callback = tf.keras.callbacks.ReduceLROnPlateau(
    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8
)

# EarlyStopping itself increases the memory requirement
# restore_best_weights will increase the memory req for large models
es_callback = tf.keras.callbacks.EarlyStopping(
    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False
)

# Train the model
t1 = time.time()
model.fit(train_ds, validation_data=valid_ds, epochs=10, class_weight=class_weight_dict, callbacks=[es_callback, lr_callback, csv_logger])
t2 = time.time()

print("It took {} seconds to complete the training".format(t2-t1))

os.makedirs('models', exist_ok=True)
tf.keras.models.save_model(model, os.path.join('models', '1_sentiment_analysis.h5'))

model.evaluate(test_ds)

# Section 9.6

import tensorflow.keras.backend as K

K.clear_session()

# Code listing 9.7
model = tf.keras.models.Sequential([

    # Adding an Embedding layer
    # You will see the following error if you don't filter out all zero (empty) records from the dataset
    # these records return a vector of all zeros which leads the LSTM layer to error out
    # CUDNN_STATUS_BAD_PARAM
    # in tensorflow/stream_executor/cuda/cuda_dnn.cc(1496):
    # 'cudnnSetRNNDataDescriptor(
    #     data_desc.get(), data_type,
    #     layout,
    #     max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill
    # )'
    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=128, mask_zero=True, input_shape=(None,)),
    # Defining an LSTM layer
    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),
    # Defining Dense layers
    tf.keras.layers.Dense(512, activation='relu'),
    # Defining a dropout layer
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')
])

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Section 9.7

print("Defining data pipelines")
batch_size = 128
train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)
valid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size,)
test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)
print('\tDone...')

os.makedirs('eval', exist_ok=True)

# Logging the performance metrics to a CSV file
csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','2_sentiment_analysis_embeddings.log'))

monitor_metric = 'val_loss'
mode = 'min' if 'loss' in monitor_metric else 'max'
print("Using metric={} and mode={} for EarlyStopping".format(monitor_metric, mode))

# Reduce LR callback
lr_callback = tf.keras.callbacks.ReduceLROnPlateau(
    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8
)

# EarlyStopping itself increases the memory requirement
# restore_best_weights will increase the memory req for large models
es_callback = tf.keras.callbacks.EarlyStopping(
    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False
)

t1 = time.time()

model.fit(train_ds, validation_data=valid_ds, epochs=10, class_weight=class_weight_dict, callbacks=[es_callback, lr_callback, csv_logger])
t2 = time.time()

print("It took {} seconds to complete the training".format(t2-t1))

os.makedirs('models', exist_ok=True)
tf.keras.models.save_model(model, os.path.join('models', '2_sentiment_analysis_embeddings.h5'))

test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)
model.evaluate(test_ds)

test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)

# Go through the test data and gather all examples
test_x = []
test_pred = []
test_y = []
for x, y in test_ds:
    test_x.append(x)
    test_pred.append(model.predict(x, verbose=0))
    test_y.append(y)

# Check the sizes
test_x = [doc for t in test_x for doc in t.numpy().tolist()]
print("X: {}".format(len(test_x)))
test_pred = tf.concat(test_pred, axis=0).numpy()
print("Pred: {}".format(test_pred.shape))
test_y = tf.concat(test_y, axis=0).numpy()
print("Y: {}".format(test_y.shape))

sorted_pred = np.argsort(test_pred.flatten())
min_pred = sorted_pred[:5]
max_pred = sorted_pred[-5:]

print("Most negative reviews\n")
print("="*50)
for i in min_pred:
    print(" ".join(tokenizer.sequences_to_texts([test_x[i]])), '\n')

print("\nMost positive reviews\n")
print("="*50)
for i in max_pred:
    print(" ".join(tokenizer.sequences_to_texts([test_x[i]])), '\n')

"""**Using the trained model to predict the sentiment of a given article**"""

# TODO: Import this from elsewhere, front-end perhaps.

input_text = '''
Climate change already worse than expected, says new UN report
The effects of warming are already driving people from their homes as seas rise, as well as killing trees and animal species. We can adapt, but also urgently need to make deep and immediate emissions cuts to head off even worse impacts, experts say.
Climate change is causing greater impacts than expected at lower temperatures than anticipated, disrupting natural systems and affecting the lives of billions of people around the world, according to the latest report from the United Nations’ Intergovernmental Panel on Climate Change (IPCC).

The new report found that droughts and heat waves are killing off trees and corals; sea level rise is forcing people in vulnerable areas to leave their homes; and extreme conditions may be increasing the likelihood of violent conflict. If warming is not halted soon, and it continues, as many as half the species living on land could become extinct, malnutrition in parts of the world will likely become widespread, and extreme weather events will become increasingly common.

The poor, the very young and very old, ethnic minorities, and Indigenous peoples are at most risk. And while measures to limit the impact of climate change do exist, the only truly meaningful step is to reduce greenhouse gas emissions as soon as possible.

According to Kelly Levin of the Bezos Earth Fund, a foundation that funds efforts to combat climate change, the report “shows clearly how much we need to change course, because delayed action risks triggering impacts that are so catastrophic that our world slowly becomes unrecognizable.”

The report, Climate Change 2022: Impacts, Adaptation and Vulnerability, was published on Monday after approval from 195 governments. It runs 3,675 pages. Here are its major findings.

Many climate change impacts are worse than thought
To date, greenhouse gas emissions have resulted in a mean global temperature increase just shy of 1.1 degrees Celsius (about 2 degrees Fahrenheit). According to Camille Parmesan of the University of Plymouth’s Marine Institute, who was co-author of the report’s 35-page “Summary for Policymakers,” “one of the most striking conclusions of our report is that we’re seeing adverse impacts that are much more widespread and much more negative than expected” at that level of temperature increase.
Of particular concern, she adds, is that this relatively small amount of warming has been enough to begin melting permafrost, drying out peatlands, and damaging forests through insect pest outbreaks and wildfires.

The report also finds that climate change is already impacting agriculture, forestry, fisheries, and aquaculture, including in North America. “Across North America, climate change has reduced agricultural productivity since 1961,” says Rachel Bezner-Kerr of the Cornell College of Agriculture and Life Sciences, who was also a co-author of the official summary.

What’s more, scientists can increasingly link extreme weather events directly to climate change. In fact, argues Michael Mann, director of the Earth System Science Center at Pennsylvania State University and author of The New Climate War: The Fight to Take Back Our Planet, current models continue to “underestimate the impact that climate change is already having on persistent weather extremes and underpredict the worsening of these impacts with additional warming.”

The report concludes that increased heat waves, droughts, and floods “are already exceeding plants’ and animals’ tolerance thresholds, driving mass mortalities in species such as trees and corals. These weather extremes are occurring simultaneously, causing cascading impacts that are increasingly difficult to manage.”

The need for action is urgent
The recognition that climate impacts are already being felt increases the urgency of limiting further warming. On land, up to 14 percent of species will likely face “very high risk of extinction” at global warming levels of 1.5°C (2.7°F), says the report; that figure increases to as much as 18 percent at 2°C (3.6°F), and all the way up to 48 percent at 5°C (9°F). (If the world’s nations keep their current promises to reduce emissions, a recent analysis found, warming will be kept under 2.5ºC.)

At 2°C or higher of warming, human food security risks due to climate change will be more severe, leading to nutrient deficiencies and malnutrition, particularly in Sub-Saharan Africa, South Asia, Central and South America, and small island states. Furthermore, as temperatures continue to climb, climate change impacts and risks are likely to become “increasingly complex and more difficult to manage,” with multiple hazards—from droughts and wildfires to sea-level rise and floods—occurring simultaneously.

It is for that reason that the goal of the Paris Agreement is to limit warming to “well below 2 degrees, and preferably 1.5 degrees” of warming. However, even many of the countries committed to 1.5°C are expecting the average global temperature to rise above that level first before dropping, a process known as overshoot. With severe impacts being felt now, such an approach may be dangerous.

“Because of all these changes already being put into motion, we are concluding that with overshoot … we have an increased risk of irreversible impacts, such as species extinctions, and also that some of these processes that we’re already seeing take place, will be increasingly difficult to reverse,” says Parmesan.

“I would argue we have to try to limit warming to 1.5°C with as little overshoot as possible, ideally zero overshoot,” says Mann. “But what we need to do is actually pretty straightforward: We need to ramp carbon emissions down as rapidly as possible.”

Climate change affects some more than others
“Climate change affects us all, but it doesn’t affect us all equally,” explains Katharine Hayhoe, chief scientist for The Nature Conservancy. “Those already living below the poverty line, the very young and very old, ethnic minorities and Indigenous peoples: these are populations disproportionately affected by climate impacts. And in many cases, they’re also the ones who have done the least to contribute to the problem. That’s why climate change is profoundly unfair.”

The report points out that vulnerability to climate change is higher in locations and among populations that have less resilience to extreme change: for example, those affected by poverty and violent conflict. While there is little evidence so far that climate change directly causes conflict, it may increase the risk of it by exacerbating social, economic, and environmental problems. From 2010 to 2020, human deaths from floods, droughts, and storms were 15 times higher in highly vulnerable regions, compared to regions with very low vulnerability. Flood and drought-related food insecurity and malnutrition have increased in Africa and Central and South America.

Climate and weather extremes are increasingly driving displacement of populations in some regions; small island states threatened by rising seas are disproportionately affected. The loss of ecosystem services has especially severe effects on those who rely directly on them to meet basic needs, including Indigenous peoples.

We can adapt; nature is key
As temperatures increase, humans need to adapt. One of the report’s key findings, says Parmesan, is that such adaptation “is more reliant on natural ecosystems than we’d seen in prior reports. There is more evidence now of that dependency.”

Flood risk along rivers can be reduced by restoring wetlands and other natural habitats in flood plains, or by returning rivers to their natural courses. Conserving mangroves protects shorelines from storms and erosion. By reducing overfishing, marine protected areas provide resilience against climate change. Cities can be cooled by parks and ponds and by greening streets and rooftops. Farmers can increase both their climate resilience and their yields by improving soil health.

Adaptation that focuses on the maintenance of natural systems also helps avoid  “maladaptation,” the report says. Irrigating fields with groundwater can  provide immediate relief from drought, for example—but if droughts become more frequent or long-lasting, the water table may ultimately disappear.  Similarly, sea walls might protect coastal areas in the short term, but their construction can destroy coastal ecosystems such as coral reefs, which themselves contribute to coastal protection.

“The worst possible maladaptation,” Mann argues, would be “to put too many marbles in the adaptation bag and not enough in the mitigation bag”—in other words, to adapt to the impacts of climate change without taking adequate steps to eliminate the greenhouse gas emissions that cause it. In a sense, he continues, “the report is a summary of what we already know: Dangerous climate change is now upon us, and it is simply a matter of how bad we’re willing to let it get.”
'''

# Predict sentiment of a given article.

import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences


def predict_sentiment(text):
    # Load the Keras model
    # model = load_model('models/2_sentiment_analysis_embeddings.h5')
    model = load_model('models/lstm.h5', custom_objects={'OnehotEncoder': OnehotEncoder})
    # model = load_model('models/lstm_embedding.h5', custom_objects={'OnehotEncoder': OnehotEncoder})

    # Define the maximum sequence length used during training
    max_sequence_length = dataframe.shape[0]  # Adjust as needed based on your model's input shape

    # Clean and preprocess the input text using your function
    processed_input = clean_text(input_text)

    # Convert text to sequences
    sequences = tokenizer.texts_to_sequences([processed_input])

    # Pad the sequences to match the model's input shape
    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)

    # Make predictions using the model
    predicted_sentiment = model.predict(padded_sequences)

    return predicted_sentiment


predicted_sentiment = predict_sentiment(input_text)


def map_to_labels(sentiment):
    # Define a threshold for classifying sentiments
    threshold = 0.5  # You can adjust this threshold as needed

    # Check if the highest probability crosses the threshold
    if np.max(predicted_sentiment) >= threshold:
        # Determine the sentiment label based on the highest probability
        predicted_sentiment_label = np.argmax(predicted_sentiment, axis=1)
        # Map the numeric label to your original labels
        sentiment_labels = {0: "negative", 1: "neutral", 2: "positive"}
        predicted_sentiment_label = sentiment_labels[predicted_sentiment_label[0]]
    else:
        # If the highest probability doesn't cross the threshold, classify as "neutral" or "positive" (or handle as needed)
        predicted_sentiment_label = "neutral"  # You can change this as needed

    return predicted_sentiment_label


# Map the predicted sentiment to labels or categories as needed
predicted_sentiment_label = map_to_labels(predicted_sentiment)


print("Predicted Sentiment:", predicted_sentiment_label, predicted_sentiment)

"""## Bag of goodies before your fun exercise

**Use OpenAI to make the article more positive**

*Need OpenAI API key of a paid account*
"""

!pip install openai tiktoken

# Rewrite negative article to make it more positive.

import openai
import tiktoken


# Initialize the OpenAI API (You need to replace 'your-api-key' with your actual API key)

# LIMITED BUDGET. Uncomment to use.
# Wait a minute if rate limited.
# openai.api_key = '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'

openai_model_name = "gpt-3.5-turbo"

# See: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
def num_tokens_from_string(string: str) -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.encoding_for_model(openai_model_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens


# Function to rewrite negative sentiment text using OpenAI API
def rewrite_text_with_openai_api(input_text):
    '''
    Use the OpenAI API to generate a more positive version of the text
    Only to be called if `input_text` is classified as negative.
    '''
    rewrite_prompt = 'Rewrite this article to make it more positive:  ' + input_text

    # Calculate the number of tokens in the rewrite_prompt
    token_count = num_tokens_from_string(rewrite_prompt)
    assert(token_count <= 3000)  # Need space for response too.

    # Should probably do error handling.
    rewritten_text = openai.ChatCompletion.create(
        model=openai_model_name,  # Chat models require the correct endpoint
        messages=[
            {"role": "system", "content": "You are a climate change journalist with a positive view on life."},
            {"role": "user", "content": rewrite_prompt},
        ],
        max_tokens=4000-token_count  # For response only.
    ).choices[0].message["content"]
    return rewritten_text


# Rewrite the text (if needed) and classify sentiment
if predicted_sentiment_label == 'negative':
    # If the predicted sentiment is negative, rewrite the text
    # Keep the original text if sentiment is not negative
    rewritten_text = rewrite_text_with_openai_api(input_text)
    predicted_sentiment_rewritten = predict_sentiment(rewritten_text)
    predicted_sentiment_rewritten_label = map_to_labels(predicted_sentiment_rewritten)

    print("Original Sentiment:", predicted_sentiment_label, predicted_sentiment)
    print("More positive rewrite below." if predicted_sentiment_rewritten[0][2] > predicted_sentiment[0][2] else "")
    print("Rewritten Sentiment:", predicted_sentiment_rewritten_label, predicted_sentiment_rewritten)
    print("Rewritten Text:", rewritten_text)