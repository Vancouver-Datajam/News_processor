{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VedVLW6y2vq5",
    "outputId": "8c0f1ec9-9a6d-4e51-a8b4-d4e593508410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import requests\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from PIL import Image\n",
    "from PIL.PngImagePlugin import PngImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from functools import partial\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "# Seeding\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation/Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVdc6SHbADxV",
    "outputId": "e850df44-5f22-4768-b9e1-a41eac636505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  sentiment\n",
      "0  âˆ’ Scope 3: Optional scope that includes indire...          1\n",
      "1  The Group is not aware of any noise pollution ...          0\n",
      "2  Global climate change could exacerbate certain...          0\n",
      "3  Setting an investment horizon is part and parc...          0\n",
      "4  Climate change the physical impacts of climate...          0\n",
      "Index(['message', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "test_file = 'data/test.parquet'\n",
    "train_file = 'data/train.parquet'\n",
    "\n",
    "test_table = pq.read_table(test_file)\n",
    "train_table = pq.read_table(train_file)\n",
    "\n",
    "train_df = train_table.to_pandas()\n",
    "test_df = test_table.to_pandas()\n",
    "dataframe = pd.concat([train_df,test_df])\n",
    "dataframe.columns = ['message', 'sentiment']\n",
    "print(dataframe.head())\n",
    "print(dataframe.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8-HV8C1HwBF",
    "outputId": "22c9ec2e-8b4d-4642-f939-483d60d66456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning up: (1320, 2)\n",
      "After cleaning up: (1320, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Cleaning up data - removing null or empty data.\n",
    "Will need to change the column header depending on the dataset (\"message\")\n",
    "'''\n",
    "print(\"Before cleaning up: {}\".format(dataframe.shape))\n",
    "dataframe = dataframe[~dataframe[\"message\"].isna()]\n",
    "dataframe = dataframe[dataframe[\"message\"].str.strip().str.len()>0]\n",
    "print(\"After cleaning up: {}\".format(dataframe.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yV6LhUQ0Jpq8",
    "outputId": "7cfffdf8-4fbf-4721-eb4b-2ca7b1cff500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               message  sentiment  label\n",
      "641  Since 2017, we have been advancing our capabil...          1      1\n",
      "80   Given the complex legal and regulatory environ...          0      0\n",
      "507  We participate in the Carbon Disclosure Projec...          1      1\n",
      "74   In October 2020, we approved our sustainable f...          2      2\n",
      "33   Where relevant, our Bank considers the social ...          1      1\n",
      "\n",
      "\n",
      "1    571\n",
      "0    448\n",
      "2    301\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Depending on what we want, we can decide on how we want to map\n",
    "the sentiments to classses for training\n",
    "\n",
    "For now, we can work off the assumption of the following\n",
    "1 and 2 -> 1\n",
    "0 and -1 -> 0\n",
    "\n",
    "The final counts are printed out below\n",
    "Skip this if working with parquet data\n",
    "'''\n",
    "# dataframe[\"label\"] = dataframe[\"sentiment\"].map({1: 1, 2: 1, 0: 0, -1: 0})\n",
    "dataframe[\"label\"] = dataframe[\"sentiment\"].map({1: 1, 2: 2, 0: 0})\n",
    "print(dataframe.head())\n",
    "print(\"\\n\")\n",
    "# Number of each sentiment in the dataset\n",
    "print( dataframe[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_uxATKYK9ke",
    "outputId": "8b70822c-ffdf-48fe-93e2-6ebded6ca2f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1320,)\n"
     ]
    }
   ],
   "source": [
    "# shuffle data\n",
    "dataframe = dataframe.sample(frac=1.0, random_state=random_seed)\n",
    "\n",
    "# split data into inputs and targets\n",
    "inputs, labels = dataframe[\"message\"], dataframe[\"label\"]\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_IX35eTLme5",
    "outputId": "4bc50cd6-d5d3-471d-fc1e-47c0954f2a6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to nltk...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to nltk...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before clean: She sells seashells by the seashore.\n",
      "After clean: ['sell', 'seashell', 'seashore']\n",
      "Data already found. If you want to rerun anyway, set rerun=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Performing the following\n",
    "Lower case (nltk) - Turn \"I am\" to \"i am\"\n",
    "Remove numbers (regex) - Turn \"i am 24 years old\" to \"i am years old\"\n",
    "Remove stop words (nltk) - Turn \"i go to the shop\" to \"i go shop\"\n",
    "Lemmatize (nltk) - Turn \"i went to buy flowers\" to \"i go to buy flower\"\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='nltk')\n",
    "nltk.download('wordnet', download_dir='nltk')\n",
    "nltk.download('stopwords', download_dir='nltk')\n",
    "nltk.download('punkt', download_dir='nltk')\n",
    "nltk.download('omw-1.4', download_dir='nltk')\n",
    "nltk.data.path.append(os.path.abspath('nltk'))\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "rerun = False\n",
    "\n",
    "# Define a lemmatizer (converts words to base form)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the English stopwords\n",
    "EN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}\n",
    "\n",
    "# Code listing 9.2\n",
    "def clean_text(doc):\n",
    "    \"\"\" A function that cleans a given document (i.e. a text string)\"\"\"\n",
    "\n",
    "    # Turn to lower case\n",
    "    doc = doc.lower()\n",
    "    # the shortened form n't is expanded to not\n",
    "    doc = re.sub(pattern=r\"\\w+n\\'t \", repl=\"not \", string=doc)\n",
    "    # shortened forms like 'll 're 'd 've are removed as they don't add much value to this task\n",
    "    doc = re.sub(r\"(?:\\'ll |\\'re |\\'d |\\'ve )\", \" \", doc)\n",
    "    # numbers are removed\n",
    "    doc = re.sub(r\"/d+\",\"\", doc)\n",
    "    # break the text in to tokens (or words), while doing that ignore stopwords from the result\n",
    "    # stopwords again do not add any value to the task\n",
    "    tokens = [w for w in word_tokenize(doc) if w not in EN_STOPWORDS and w not in string.punctuation]\n",
    "\n",
    "    # Here we lemmatize the words in the tokens\n",
    "    # to lemmatize, we get the pos tag of each token and\n",
    "    # if it is N (noun) or V (verb) we lemmatize, else\n",
    "    # keep the original form\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    clean_text = [\n",
    "        lemmatizer.lemmatize(w, pos=p[0].lower()) \\\n",
    "        if p[0]=='N' or p[0]=='V' else w \\\n",
    "        for (w, p) in pos_tags\n",
    "    ]\n",
    "\n",
    "    # return the clean text\n",
    "    return clean_text\n",
    "\n",
    "if rerun or \\\n",
    "    not os.path.exists('sentiment_inputs.pkl') or \\\n",
    "    not os.path.exists('sentiment_labels.pkl'):\n",
    "    # Apply the transformation to the full text\n",
    "    # this is time consuming\n",
    "    print(\"\\nProcessing all the review data ... This can take some time (several minutes)\")\n",
    "    inputs = inputs.apply(lambda x: clean_text(x))\n",
    "    print(\"\\tDone\")\n",
    "\n",
    "    print(\"Saving the data\")\n",
    "    inputs.to_pickle('sentiment_inputs.pkl')\n",
    "    labels.to_pickle('sentiment_labels.pkl')\n",
    "\n",
    "else:\n",
    "    # Load the data from the disk\n",
    "    print(\"Data already found. If you want to rerun anyway, set rerun=True\")\n",
    "    inputs = pd.read_pickle( 'sentiment_inputs.pkl')\n",
    "    labels = pd.read_pickle( 'sentiment_labels.pkl')\n",
    "    # labels = tf.keras.utils.to_categorical(labels, num_classes=3)\n",
    "    # print(labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juunXIRjMb2G",
    "outputId": "39c8808b-aa85-4532-df99-6cfb1ddd27da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: R$1.745 billion) in bonds distributed in the U.S. market and maturing in 2027. In addition to the green bonds mentioned above, we carried out a social and environmental due diligence process to enable the issue of R$7.67 billion in bonds to be used for long-term investments in specific projects.\n",
      "Clean: ['r', '1.745', 'billion', 'bond', 'distribute', 'u.s.', 'market', 'mature', '2027.', 'addition', 'green', 'bond', 'mention', 'carry', 'social', 'environmental', 'due', 'diligence', 'process', 'enable', 'issue', 'r', '7.67', 'billion', 'bond', 'use', 'long-term', 'investment', 'specific', 'project']\n",
      "\n",
      "\n",
      "Actual: Scope 3 Data Centres Scope 3 Data Centres Greenhouse Gas Emissions (Australia operations) relate to the electricity and diesel Greenhouse Gas Emissions consumption in our Australian data centres not under our operational control as defined under NGER. CBA has (Australia operations) not had operational control of any data centres since FY18. Source of emissions factors: NGA (2018).\n",
      "Clean: ['scope', '3', 'data', 'centre', 'scope', '3', 'data', 'centre', 'greenhouse', 'gas', 'emission', 'australia', 'operation', 'relate', 'electricity', 'diesel', 'greenhouse', 'gas', 'emission', 'consumption', 'australian', 'data', 'centre', 'not', 'operational', 'control', 'define', 'nger', 'cba', 'australia', 'operation', 'not', 'operational', 'control', 'data', 'centre', 'since', 'fy18', 'source', 'emission', 'factor', 'nga', '2018']\n",
      "\n",
      "\n",
      "Actual: ISO 14001 (the international standard for environmental management systems) tailored specifically for airlines and is fully compatible with the International Organisation for Standardisation (ISO). British Airways achieved Stage 1 certification in 2019 and all other Group airlines are progressing on Stage 1 certification in 2020.\n",
      "Clean: ['iso', '14001', 'international', 'standard', 'environmental', 'management', 'system', 'tailor', 'specifically', 'airline', 'fully', 'compatible', 'international', 'organisation', 'standardisation', 'iso', 'british', 'airway', 'achieve', 'stage', '1', 'certification', '2019', 'group', 'airline', 'progress', 'stage', '1', 'certification', '2020']\n",
      "\n",
      "\n",
      "Actual: Should oil and gas prices remain at current levels or continue to decline we expect, in addition to the direct impact on the value of our oil and gas assets, there may be negative impacts on our other investments (including our debt and real estate portfolio) which are difficult to estimate.\n",
      "Clean: ['oil', 'gas', 'price', 'remain', 'current', 'level', 'continue', 'decline', 'expect', 'addition', 'direct', 'impact', 'value', 'oil', 'gas', 'asset', 'may', 'negative', 'impact', 'investment', 'include', 'debt', 'real', 'estate', 'portfolio', 'difficult', 'estimate']\n",
      "\n",
      "\n",
      "Actual: The sectors most negatively impacted by 2050 as a result of carbon pricing and related policies include fossil fuel intensive industries such as coal mining, carbon intensive power generation, and oil and gas exploration and production. The utilities sector experiences a high variation in impact, with the least carbon intensive companies experiencing positive growth. Within the transport sectors, air transportation is more affected than shipping, with road and rail transportation least affected due to the relative availability of electrification.\n",
      "Clean: ['sector', 'negatively', 'impacted', '2050', 'result', 'carbon', 'pricing', 'related', 'policy', 'include', 'fossil', 'fuel', 'intensive', 'industry', 'coal', 'mine', 'carbon', 'intensive', 'power', 'generation', 'oil', 'gas', 'exploration', 'production', 'utility', 'sector', 'experience', 'high', 'variation', 'impact', 'least', 'carbon', 'intensive', 'company', 'experience', 'positive', 'growth', 'within', 'transport', 'sector', 'air', 'transportation', 'affect', 'ship', 'road', 'rail', 'transportation', 'least', 'affected', 'due', 'relative', 'availability', 'electrification']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for actual, clean in zip(dataframe[\"message\"].iloc[:5], inputs.iloc[:5]):\n",
    "  print(\"Actual: {}\".format(actual))\n",
    "  print(\"Clean: {}\".format(clean))\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhBhz0w6eNcp"
   },
   "source": [
    "**Data Vizualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "4E35L4CBeM25",
    "outputId": "9669b7bf-933f-440e-e512-69d079f35300"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-64f6f229c5a2>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Extract the top 25 words in each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mnews_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmatized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'News'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Extract the words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# change sentiment from int to words for easy read\n",
    "df = dataframe.copy()\n",
    "sentiment = df['sentiment']\n",
    "word_sentiment = []\n",
    "\n",
    "for i in sentiment :\n",
    "    if i == 2 :\n",
    "        word_sentiment.append('Positive')\n",
    "    elif i == 1 :\n",
    "        word_sentiment.append('Neutral')\n",
    "    elif i == 0 :\n",
    "        word_sentiment.append('Negative')\n",
    "    else :\n",
    "            word_sentiment.append('News')\n",
    "\n",
    "\n",
    "df['sentiment'] = word_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiyPJrtIMs0A"
   },
   "outputs": [],
   "source": [
    "def frequency(news):\n",
    "    \"\"\"\n",
    "    This function determines the frequency of each word in our dataset\n",
    "    and stores the most frequent words in a dataframe,\n",
    "    sorted from most to least frequent\n",
    "    \"\"\"\n",
    "\n",
    "    # Count vectorizer excluding english stopwords\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    words = cv.fit_transform(news)\n",
    "\n",
    "    # Count the words in the news and determine the frequency of each word\n",
    "    sum_words = words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Create a dataframe to store the top 30 words and their frequencies\n",
    "    frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
    "    frequency = frequency.head(30)\n",
    "\n",
    "    return frequency\n",
    "\n",
    "\n",
    "# Extract the words\n",
    "news_frequency = frequency(df['message'])\n",
    "pro_frequency = frequency(df['message'][df['sentiment']=='Positive'])\n",
    "anti_frequency = frequency(df['message'][df['sentiment']=='Negative'])\n",
    "neutral_frequency = frequency(df['message'][df['sentiment']=='Neutral'])\n",
    "\n",
    "news_words = ' '.join([text for text in news_frequency['word']])\n",
    "anti_words = ' '.join([text for text in anti_frequency['word']])\n",
    "pro_words = ' '.join([text for text in pro_frequency['word']])\n",
    "neutral_words = ' '.join([text for text in neutral_frequency['word']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRvoJpEhMvsD"
   },
   "outputs": [],
   "source": [
    "# Create wordcloud for the climate change classes\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords_custom = {\"year\", \"including\", \"related\"}\n",
    "stopwords.update(stopwords_custom)\n",
    "\n",
    "anti_wordcloud = WordCloud(width=800,\n",
    "                           height=500,\n",
    "                           random_state=110,\n",
    "                           max_font_size=110,\n",
    "                           background_color='white',\n",
    "                           stopwords = stopwords,\n",
    "                           colormap=\"Reds\").generate(anti_words)\n",
    "\n",
    "\n",
    "pro_wordcloud = WordCloud(width=800,\n",
    "                          height=500,\n",
    "                          random_state=73,\n",
    "                          max_font_size=110,\n",
    "                          background_color='white',\n",
    "                          stopwords = stopwords,\n",
    "                          colormap=\"Greens\").generate(pro_words)\n",
    "\n",
    "\n",
    "news_wordcloud = WordCloud(width=800,\n",
    "                          height=500,\n",
    "                          random_state=0,\n",
    "                          max_font_size=110,\n",
    "                          background_color='white',\n",
    "                          stopwords = stopwords,\n",
    "                          colormap=\"Blues\").generate(news_words)\n",
    "\n",
    "\n",
    "neutral_wordcloud = WordCloud(width=800,\n",
    "                          height=500,\n",
    "                          random_state=10,\n",
    "                          max_font_size=110,\n",
    "                          background_color='white',\n",
    "                          stopwords = stopwords,\n",
    "                          colormap=\"Oranges\").generate(neutral_words)\n",
    "\n",
    "\n",
    "# Plot pro and anti wordclouds next to one another for comparisson\n",
    "f, axarr = plt.subplots(2,2, figsize=(35,25))\n",
    "axarr[0,0].imshow(pro_wordcloud, interpolation=\"bilinear\")\n",
    "axarr[0,1].imshow(anti_wordcloud, interpolation=\"bilinear\")\n",
    "axarr[1,0].imshow(neutral_wordcloud, interpolation=\"bilinear\")\n",
    "axarr[1,1].imshow(news_wordcloud, interpolation=\"bilinear\")\n",
    "\n",
    "# Remove the ticks on the x and y axes\n",
    "for ax in f.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.axis('off')\n",
    "\n",
    "axarr[0,0].set_title('Positive\\n', fontsize=35)\n",
    "axarr[0,1].set_title('Negative \\n', fontsize=35)\n",
    "axarr[1,0].set_title('Neutral\\n', fontsize=35)\n",
    "axarr[1,1].set_title('News\\n', fontsize=35)\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgHnG-tmMy-3"
   },
   "outputs": [],
   "source": [
    "# Create wordcolud for the news climate change dataframe\n",
    "news_wordcloud = WordCloud(width=800,\n",
    "                          height=500,\n",
    "                          random_state=0,\n",
    "                          max_font_size=110,\n",
    "                          background_color='white',\n",
    "                          stopwords = stopwords,\n",
    "                          colormap=\"Blues\").generate(news_words)\n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(news_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SKEERU3M4A8"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Downloads\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def entity_extractor(news):\n",
    "\n",
    "    \"\"\"\n",
    "    This function extracts the top 10 people, organizations and geopolitical entities\n",
    "    in a collection of news.\n",
    "    The information is then saved in a new dataframe\n",
    "\n",
    "    Input:\n",
    "    news: lemmatized news\n",
    "           datatype: 'str'\n",
    "\n",
    "    Output:\n",
    "    df: dataframe containing the top 10 people, organizations and gpe's in a collection of news\n",
    "        datatype: dataframe ('str')\n",
    "    \"\"\"\n",
    "\n",
    "    def get_people(news):\n",
    "        words = nlp(news)\n",
    "        people = [w.text for w in words.ents if w.label_== 'PERSON']\n",
    "        return people\n",
    "\n",
    "    def get_org(news):\n",
    "        words = nlp(news)\n",
    "        org = [w.text for w in words.ents if w.label_== 'ORG']\n",
    "        return org\n",
    "\n",
    "    def get_gpe(news):\n",
    "        words = nlp(news)\n",
    "        gpe = [w.text for w in words.ents if w.label_== 'GPE']\n",
    "        return gpe\n",
    "\n",
    "    # Extract the top 10 people\n",
    "    people = news.apply(lambda x: get_people(x))\n",
    "    people = [x for sub in people for x in sub]\n",
    "    people_counter = Counter(people)\n",
    "    people_count = people_counter.most_common(10)\n",
    "    people, people_count = map(list, zip(*people_count))\n",
    "\n",
    "    # Extract the top 10 organizations\n",
    "    org = news.apply(lambda x: get_org(x))\n",
    "    org = [x for sub in org for x in sub]\n",
    "    org_counter = Counter(org)\n",
    "    org_count = org_counter.most_common(10)\n",
    "    org, org_count = map(list, zip(*org_count))\n",
    "\n",
    "    # Extract the top 10 geopolitical entities\n",
    "    gpe = news.apply(lambda x: get_gpe(x))\n",
    "    gpe = [x for sub in gpe for x in sub]\n",
    "    gpe_counter = Counter(gpe)\n",
    "    gpe_count = gpe_counter.most_common(10)\n",
    "    gpe, gpe_count = map(list, zip(*gpe_count))\n",
    "\n",
    "    # Create a dataframe to store the information\n",
    "    df = pd.DataFrame({'people' : people})\n",
    "    df['geopolitics'] = gpe\n",
    "    df['organizations'] = org\n",
    "\n",
    "    return df\n",
    "\n",
    "# Extract top entities for each class\n",
    "anti_info = entity_extractor(df['message'][df['sentiment']=='Negative'])\n",
    "pro_info = entity_extractor(df['message'][df['sentiment']=='Positive'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBM4c9LKM5yP"
   },
   "outputs": [],
   "source": [
    "print('Positive climate change information')\n",
    "display(pro_info.head(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHWJc-sMM75G"
   },
   "outputs": [],
   "source": [
    "print('Negative climate change information')\n",
    "display(anti_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5EaRHL2eWst"
   },
   "source": [
    "**Model Exploration and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvVSvgq0Nfu8",
    "outputId": "955e433c-84b0-4b87-f2ab-39ca073b2f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 1649\n",
      "Validation data: 129\n",
      "Test data: 132\n"
     ]
    }
   ],
   "source": [
    "def train_valid_test_split(inputs, labels, train_fraction=0.8):\n",
    "    \"\"\" Splits a given dataset into three sets; training, validation and test \"\"\"\n",
    "\n",
    "    # Separate indices of negative and positive data points\n",
    "    neg_indices = pd.Series(labels.loc[(labels==0)].index)\n",
    "    neut_indices = pd.Series(labels.loc[(labels==1)].index)\n",
    "    pos_indices = pd.Series(labels.loc[(labels==2)].index)\n",
    "\n",
    "    n_valid = int(min([len(neg_indices), len(neut_indices), len(pos_indices)]) * ((1-train_fraction)/2.0))\n",
    "    n_test = n_valid\n",
    "\n",
    "    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)\n",
    "    neg_valid_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    neg_train_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds.tolist()+neg_valid_inds.tolist())]\n",
    "\n",
    "    neut_test_inds = neut_indices.sample(n=n_test, random_state=random_seed)\n",
    "    neut_valid_inds = neut_indices.loc[~neut_indices.isin(neut_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    neut_train_inds = neut_indices.loc[~neut_indices.isin(neut_test_inds.tolist()+neut_valid_inds.tolist())]\n",
    "\n",
    "    pos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed)\n",
    "    pos_valid_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    pos_train_inds = pos_indices.loc[\n",
    "        ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())\n",
    "    ]\n",
    "\n",
    "    tr_x = inputs.loc[neg_train_inds.tolist() + neut_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    tr_y = labels.loc[neg_train_inds.tolist() + neut_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_x = inputs.loc[neg_valid_inds.tolist() + neut_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_y = labels.loc[neg_valid_inds.tolist() + neut_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_x = inputs.loc[neg_test_inds.tolist() + neut_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_y = labels.loc[neg_test_inds.tolist() + neut_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "\n",
    "    # tr_y = pd.get_dummies(tr_y, columns=['label'], prefix='class')\n",
    "    # v_y = pd.get_dummies(v_y, columns=['label'], prefix='class')\n",
    "    # ts_y = pd.get_dummies(ts_y, columns=['label'], prefix='class')\n",
    "\n",
    "    print('Training data: {}'.format(len(tr_x)))\n",
    "    print('Validation data: {}'.format(len(v_x)))\n",
    "    print('Test data: {}'.format(len(ts_x)))\n",
    "\n",
    "    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)\n",
    "\n",
    "(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7l198HxhNjI_",
    "outputId": "ce78f81e-cb7f-4f48-df67-e8bf59e22b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sample targets\n",
      "191    1\n",
      "232    1\n",
      "781    1\n",
      "215    0\n",
      "84     2\n",
      "Name: label, dtype: int64\n",
      "191    [scope, 1, emission, come, mostly, refrigerant...\n",
      "232    [2019, create, role, dedicate, enhance, verizo...\n",
      "781    [september, 2018, together, index, initiative,...\n",
      "215    [group, level, 2019, 61, spill, hazardous, mat...\n",
      "84     [new, product, include, 1, per, day, electric,...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Some sample targets\")\n",
    "print(tr_y.head())\n",
    "print(tr_x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rji1wdyRNpCW",
    "outputId": "2c4f5730-e0b3-4858-f34c-882e201e7e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk             1260\n",
      "climate          1017\n",
      "change            857\n",
      "emission          709\n",
      "energy            622\n",
      "impact            606\n",
      "â€™                 559\n",
      "include           521\n",
      "business          510\n",
      "environmental     506\n",
      "dtype: int64\n",
      "count    6501.000000\n",
      "mean       12.068605\n",
      "std        41.939369\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%         7.000000\n",
      "max      1260.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Analysis of vocabulary\n",
    "'''\n",
    "from collections import Counter\n",
    "# Create a large list which contains all the words in all the reviews\n",
    "data_list = [w for doc in tr_x for w in doc]\n",
    "\n",
    "# Create a Counter object from that list\n",
    "# Counter returns a dictionary, where key is a word and the value is the frequency\n",
    "cnt = Counter(data_list)\n",
    "\n",
    "# Convert the result to a pd.Series\n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "# Print most common words\n",
    "print(freq_df.head(n=10))\n",
    "\n",
    "# Print summary statistics\n",
    "print(freq_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyJu7jMDNxCn",
    "outputId": "267ce7b5-d6d1-42e1-d26f-a66adf88c123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some summary statistics\n",
      "Median length: 42.0\n",
      "\n",
      "\n",
      "Computing the statistics between the 10% and 90% quantiles (to ignore outliers)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1333.000000\n",
       "mean       43.171793\n",
       "std        11.738698\n",
       "min        25.000000\n",
       "33%        36.000000\n",
       "50%        42.000000\n",
       "66%        47.000000\n",
       "max        72.000000\n",
       "Name: message, dtype: float64"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Analysing thesequence length (number of words) of news\n",
    "'''\n",
    "# Create a pd.Series, which contain the sequence length for each review\n",
    "seq_length_ser = tr_x.str.len()\n",
    "\n",
    "# Get the median as well as summary statistics of the sequence length\n",
    "print(\"\\nSome summary statistics\")\n",
    "print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "seq_length_ser.describe()\n",
    "\n",
    "print(\"\\nComputing the statistics between the 10% and 90% quantiles (to ignore outliers)\")\n",
    "p_10 = seq_length_ser.quantile(0.1)\n",
    "p_90 = seq_length_ser.quantile(0.9)\n",
    "\n",
    "seq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe(percentiles=[0.33, 0.66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQ5M0xZPN-w2",
    "outputId": "2a30416d-25dd-44c8-dd17-da252e9c28ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a vocabulary of size: 1323\n"
     ]
    }
   ],
   "source": [
    "n_vocab = (freq_df >= 10).sum()\n",
    "print(\"Using a vocabulary of size: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d-bVWLTPH0m"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Transforming text to numbers\n",
    "'''\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a tokenizer that will convert words to IDs\n",
    "# words that are less frequent will be replaced by 'unk'\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "\n",
    "# Fit the tokenizer on the data\n",
    "tokenizer.fit_on_texts(tr_x.tolist())\n",
    "\n",
    "# Convert all of train/validation/test data to sequences of IDs\n",
    "tr_x = tokenizer.texts_to_sequences(tr_x.tolist())\n",
    "v_x = tokenizer.texts_to_sequences(v_x.tolist())\n",
    "ts_x = tokenizer.texts_to_sequences(ts_x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hxnRkmhPXvH",
    "outputId": "730e05b8-d6f7-4e2e-db44-419493aa0630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word id for \"climate\" is: 3\n",
      "The word for id 4 is: change\n"
     ]
    }
   ],
   "source": [
    "# Checking the attributes of the tokenizer\n",
    "word = \"climate\"\n",
    "wid = tokenizer.word_index[word]\n",
    "print(\"The word id for \\\"{}\\\" is: {}\".format(word, wid))\n",
    "wid = 4\n",
    "word = tokenizer.index_word[wid]\n",
    "print(\"The word for id {} is: {}\".format(wid, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5W_YNYiPdTx",
    "outputId": "7b1026a3-be52-4499-f98d-57e42855ff8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['environmental', 'management', 'system', 'encompass', 'series', 'metric', 'related', 'greenhouse', 'gas', 'emission', 'capture', 'forest', 'restoration', 'water', 'withdrawal', 'use', 'energy', 'use', 'generation', 'well', 'waste', 'generation', 'disposal']\n",
      "Sequence: [11, 25, 90, 1, 865, 271, 177, 115, 22, 5, 1192, 468, 1, 104, 1, 23, 6, 23, 118, 72, 212, 118, 1223]\n",
      "\n",
      "\n",
      "Text: ['target', 'consume', '100', 'renewable', 'electricity', 'july', '2021', 'europe', '2025', 'market']\n",
      "Sequence: [30, 778, 366, 50, 73, 861, 430, 652, 461, 69]\n",
      "\n",
      "\n",
      "Text: ['january', '2020', 'new', 'estimation', 'method', 'report', 'unaccounted', 'gas', 'uafg', 'emission', 'result', 'natural', 'gas', 'distribution', 'activity', 'introduce', 'australia', 'approach', 'enables', 'site/network', 'specific', 'uafg', 'value', 'use', 'allow', 'us', 'translate', 'network', 'maintenance', 'replacement', 'activity', 'reportable', 'reduction', 'uafg', 'emission']\n",
      "Sequence: [746, 67, 33, 1, 767, 39, 1, 22, 1, 5, 31, 74, 22, 425, 58, 554, 390, 162, 1, 1, 228, 1, 117, 23, 380, 82, 1, 268, 766, 1176, 58, 1, 95, 1, 5]\n",
      "\n",
      "\n",
      "Text: ['future', 'work', 'climate', 'continue', 'high', 'ap2', \"'s\", 'agenda', 'implementation', 'task', 'force', 'climate-related', 'financial', 'disclosure', 'part', 'fund', \"'s\", 'on-going', 'work', '2020', 'focus', 'implement', 'new', 'sustainability', 'strategy', 'strong', 'focus', 'climate', 'among', 'thing', 'fund', 'develop', 'internal', 'index', 'sustainability', 'profile', 'ambition', 'comply', 'criterion', 'eu', 'paris', 'align', 'benchmark', 'asset', 'management', 'work', 'actively', 'different', 'way', 'include', 'climate', 'risk', 'opportunity', 'analysis', 'find', 'investment', 'opportunity', 'different', 'asset', 'class', 'integrate', 'climate', 'analysis', 'overall', 'alm', 'analysis', 'continue', 'develo-', 'ped', 'fund', 'also', 'intend', 'develop', 'view', 'significant', 'climate', 'risk', 'opportunity', 'asset', 'classes/sectors/', 'geography', 'time', 'horizon']\n",
      "Sequence: [87, 75, 3, 91, 264, 1, 44, 1, 719, 578, 431, 60, 21, 227, 199, 63, 44, 1, 75, 67, 98, 283, 33, 38, 55, 756, 98, 3, 277, 1, 63, 112, 245, 335, 38, 769, 703, 839, 548, 1191, 255, 291, 887, 41, 25, 75, 749, 573, 347, 9, 3, 2, 42, 130, 550, 12, 42, 573, 41, 1267, 316, 3, 130, 249, 1, 130, 91, 1, 1, 63, 16, 661, 112, 1048, 83, 3, 2, 42, 41, 1, 1, 150, 1104]\n",
      "\n",
      "\n",
      "Text: ['however', 'factor', 'may', 'lead', 'commencement', 'engagement', 'decide', 'particularly', 'focus', 'company', 'relation', 'particular', 'esg-related', 'concern', 'not', 'publish', 'adequate', 'environmental', 'information', 'â€˜', 'laggard', 'â€™', 'regard', 'commitment', 'address', 'climate', 'change', 'issue']\n",
      "Sequence: [339, 102, 18, 96, 1, 290, 987, 330, 98, 15, 642, 361, 1, 315, 20, 604, 1, 11, 235, 382, 1, 8, 256, 131, 180, 3, 4, 71]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert words to IDs\n",
    "\n",
    "# Vocabs not used get assigned id of 1 I think, so since they're not being\n",
    "# used in the training, it doesn't actually matter\n",
    "test_text = [\n",
    "    ['environmental', 'management', 'system', 'encompass', 'series', 'metric', 'related', 'greenhouse', 'gas', 'emission', 'capture', 'forest', 'restoration', 'water', 'withdrawal', 'use', 'energy', 'use', 'generation', 'well', 'waste', 'generation', 'disposal'],\n",
    "    ['target', 'consume', '100', 'renewable', 'electricity', 'july', '2021', 'europe', '2025', 'market'],\n",
    "    ['january', '2020', 'new', 'estimation', 'method', 'report', 'unaccounted', 'gas', 'uafg', 'emission', 'result', 'natural', 'gas', 'distribution', 'activity', 'introduce', 'australia', 'approach', 'enables', 'site/network', 'specific', 'uafg', 'value', 'use', 'allow', 'us', 'translate', 'network', 'maintenance', 'replacement', 'activity', 'reportable', 'reduction', 'uafg', 'emission'],\n",
    "    ['future', 'work', 'climate', 'continue', 'high', 'ap2', \"'s\", 'agenda', 'implementation', 'task', 'force', 'climate-related', 'financial', 'disclosure', 'part', 'fund', \"'s\", 'on-going', 'work', '2020', 'focus', 'implement', 'new', 'sustainability', 'strategy', 'strong', 'focus', 'climate', 'among', 'thing', 'fund', 'develop', 'internal', 'index', 'sustainability', 'profile', 'ambition', 'comply', 'criterion', 'eu', 'paris', 'align', 'benchmark', 'asset', 'management', 'work', 'actively', 'different', 'way', 'include', 'climate', 'risk', 'opportunity', 'analysis', 'find', 'investment', 'opportunity', 'different', 'asset', 'class', 'integrate', 'climate', 'analysis', 'overall', 'alm', 'analysis', 'continue', 'develo-', 'ped', 'fund', 'also', 'intend', 'develop', 'view', 'significant', 'climate', 'risk', 'opportunity', 'asset', 'classes/sectors/', 'geography', 'time', 'horizon'],\n",
    "    ['however', 'factor', 'may', 'lead', 'commencement', 'engagement', 'decide', 'particularly', 'focus', 'company', 'relation', 'particular', 'esg-related', 'concern', 'not', 'publish', 'adequate', 'environmental', 'information', 'â€˜', 'laggard', 'â€™', 'regard', 'commitment', 'address', 'climate', 'change', 'issue'],\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "for text, seq in zip(test_text, test_seq):\n",
    "    print(\"Text: {}\".format(text))\n",
    "    print(\"Sequence: {}\".format(seq))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIm7qNsWQnbr"
   },
   "outputs": [],
   "source": [
    "def get_tf_pipeline(text_seq, labels, batch_size=64, bucket_boundaries=[5,15], max_length=50, shuffle=False):\n",
    "    \"\"\" Define a data pipeline that converts sequences to batches of data \"\"\"\n",
    "\n",
    "    num_classes = 3\n",
    "    # Concatenate the label and the input sequence so that we don't mess up the order when we shuffle\n",
    "    data_seq = [[b] + a for a, b in zip(text_seq, labels)]\n",
    "    # Define the variable sequence dataset as a ragged tensor\n",
    "    tf_data = tf.ragged.constant(data_seq)[:, :max_length]\n",
    "\n",
    "    # One-hot encode labels\n",
    "    labels = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n",
    "\n",
    "    # Create a dataset out of the ragged tensor\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices((tf_data, labels))\n",
    "    text_ds = text_ds.filter(lambda x, y: tf.size(x) > 1)\n",
    "\n",
    "    # Bucketing the data\n",
    "    # Bucketing assigns each sequence to a bucket depending on the length\n",
    "    # If you define bucket boundaries as [5, 15], then you get buckets,\n",
    "    # [0, 5], [5, 15], [15,inf]\n",
    "    bucket_fn = tf.data.experimental.bucket_by_sequence_length(\n",
    "        lambda x, y: tf.cast(tf.shape(x)[0], 'int32'),\n",
    "        bucket_boundaries=bucket_boundaries,\n",
    "        bucket_batch_sizes=[batch_size, batch_size, batch_size],\n",
    "        padded_shapes=(tf.TensorShape([max_length]), tf.TensorShape([num_classes])),\n",
    "        padding_values=(0, 0),\n",
    "        pad_to_bucket_boundary=False\n",
    "    )\n",
    "\n",
    "    # Apply bucketing\n",
    "    text_ds = text_ds.map(lambda x, y: (x, y)).apply(bucket_fn)\n",
    "\n",
    "    # Shuffle the data\n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=10 * batch_size)\n",
    "\n",
    "    return text_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nu6p8xJXQwnK",
    "outputId": "f6e468a1-d051-4cb8-ae9d-6decfe13145c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some training data ...\n",
      "Input sequence shape: (64, 50)\n",
      "tf.Tensor(\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]], shape=(64, 3), dtype=int32)\n",
      "Input sequence shape: (64, 50)\n",
      "tf.Tensor(\n",
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]], shape=(64, 3), dtype=int32)\n",
      "\n",
      "Some validation data ...\n",
      "Input sequence shape: (64, 50)\n",
      "tf.Tensor(\n",
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]], shape=(64, 3), dtype=int32)\n",
      "Input sequence shape: (64, 50)\n",
      "tf.Tensor(\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]], shape=(64, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''Testing the bucketing function '''\n",
    "train_ds = get_tf_pipeline(tr_x, tr_y, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y)\n",
    "\n",
    "print(\"Some training data ...\")\n",
    "for x,y in train_ds.take(2):\n",
    "    print(\"Input sequence shape: {}\".format(x.shape))\n",
    "    print(y)\n",
    "\n",
    "print(\"\\nSome validation data ...\")\n",
    "for x,y in valid_ds.take(2):\n",
    "    print(\"Input sequence shape: {}\".format(x.shape))\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyhiKfDhQ7DG",
    "outputId": "37f176a0-3dcf-413c-e17e-72091e8609f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda (Lambda)             (None, None, 1)           0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 1)           0         \n",
      "                                                                 \n",
      " onehot_encoder (OnehotEnco  (None, None, 1323)        0         \n",
      " der)                                                            \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               743424    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               66048     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 811011 (3.09 MB)\n",
      "Trainable params: 811011 (3.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "class OnehotEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(OnehotEncoder, self).__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        inputs = tf.cast(inputs, 'int32')\n",
    "\n",
    "        if len(inputs.shape) == 3:\n",
    "            inputs = inputs[:,:,0]\n",
    "        return tf.one_hot(inputs, depth=self.depth)\n",
    "\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'depth': self.depth})\n",
    "        return config\n",
    "\n",
    "# You will see the following error if you don't filter out all zero (empty) records from the dataset\n",
    "# these records return a vector of all zeros which leads the LSTM layer to error out\n",
    "# CUDNN_STATUS_BAD_PARAM\n",
    "# in tensorflow/stream_executor/cuda/cuda_dnn.cc(1496):\n",
    "# 'cudnnSetRNNDataDescriptor(\n",
    "#     data_desc.get(), data_type,\n",
    "#     layout,\n",
    "#     max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill\n",
    "# )'\n",
    "\n",
    "# Code listing 9.5\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=(None,)),\n",
    "    # Create a mask to mask out zero inputs\n",
    "    tf.keras.layers.Masking(mask_value=0),\n",
    "    # After creating the mask, convert inputs to onehot encoded inputs\n",
    "    OnehotEncoder(depth=n_vocab),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
    "    # tf.keras.layers.Flatten(),\n",
    "    # Defining a Dense layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4JMMAf_RE40",
    "outputId": "daa4843b-f7e7-452f-a609-09f11d05d164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking layer's mask\n",
      "tf.Tensor(\n",
      "[[ True  True  True False False]\n",
      " [ True  True  True  True False]], shape=(2, 5), dtype=bool)\n",
      "Onehot encoder layer's mask\n",
      "tf.Tensor(\n",
      "[[ True  True  True False False]\n",
      " [ True  True  True  True False]], shape=(2, 5), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "'''Checking the mask '''\n",
    "inp = tf.expand_dims(tf.constant([[2,3,4,0,0], [2,4,6,12,0]], dtype='int32'),axis=-1)\n",
    "# Create a mask to mask out zero inputs\n",
    "mask_out = tf.keras.layers.Masking(mask_value=0)(inp)\n",
    "print(\"Masking layer's mask\")\n",
    "print(mask_out._keras_mask)\n",
    "# After creating the mask, convert inputs to onehot encoded inputs\n",
    "onehot_out = OnehotEncoder(depth=10)(mask_out)\n",
    "print(\"Onehot encoder layer's mask\")\n",
    "print(onehot_out._keras_mask)\n",
    "# Defining an LSTM layer\n",
    "lstm_out = tf.keras.layers.LSTM(24, return_state=False, return_sequences=False)(\n",
    "    onehot_out, mask=onehot_out._keras_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZdk8WG8RMiy",
    "outputId": "12d09b80-d580-4fc9-de1d-05c9350984e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining data pipelines\n",
      "\tDone...\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining data pipelines\")\n",
    "\n",
    "# Using a batch size of 128\n",
    "batch_size = 128\n",
    "train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size)\n",
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)\n",
    "print('\\tDone...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gm_3Jv-xRRpZ",
    "outputId": "cb30f028-9c6c-4b44-bb8a-76934a6bcfb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.9115533443891652, 1: 0.7908872901678657, 2: 1.5660018993352327}\n"
     ]
    }
   ],
   "source": [
    "# There is a class imbalance in the data therefore we are defining a weight for negative inputs\n",
    "# neg_weight = (tr_y==1).sum()/(tr_y==0).sum()\n",
    "# print(\"Will be using a weight of {} for negative samples\".format(neg_weight))\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming you have three classes: 0, 1, and 2\n",
    "classes = [0,1,2]\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=tr_y)\n",
    "class_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n",
    "print(\"Class Weights:\", class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCIS-PApRVAj",
    "outputId": "49dc17a3-8407-4d92-88f2-d1690ca1dc5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using metric=val_loss and mode=min for EarlyStopping\n",
      "Epoch 1/10\n",
      "14/14 [==============================] - 16s 657ms/step - loss: 1.0867 - accuracy: 0.3535 - val_loss: 1.0576 - val_accuracy: 0.5194 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 7s 501ms/step - loss: 0.8507 - accuracy: 0.5306 - val_loss: 0.7707 - val_accuracy: 0.6822 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 7s 461ms/step - loss: 0.6092 - accuracy: 0.6574 - val_loss: 0.6290 - val_accuracy: 0.6899 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 8s 532ms/step - loss: 0.4369 - accuracy: 0.8065 - val_loss: 0.7874 - val_accuracy: 0.6899 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 8s 560ms/step - loss: 0.3043 - accuracy: 0.8848 - val_loss: 0.5429 - val_accuracy: 0.7907 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 7s 515ms/step - loss: 0.1552 - accuracy: 0.9466 - val_loss: 0.7064 - val_accuracy: 0.7907 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 6s 429ms/step - loss: 0.1196 - accuracy: 0.9636 - val_loss: 0.7114 - val_accuracy: 0.7829 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 8s 535ms/step - loss: 0.0792 - accuracy: 0.9685 - val_loss: 0.5906 - val_accuracy: 0.8450 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 7s 526ms/step - loss: 0.0330 - accuracy: 0.9921 - val_loss: 0.5919 - val_accuracy: 0.8605 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 7s 456ms/step - loss: 0.0270 - accuracy: 0.9945 - val_loss: 0.6403 - val_accuracy: 0.8450 - lr: 1.0000e-04\n",
      "It took 104.90953421592712 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "# Section 9.5\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','1_sentiment_analysis.log'))\n",
    "\n",
    "monitor_metric = 'val_loss'\n",
    "mode = 'min'\n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "t1 = time.time()\n",
    "model.fit(train_ds, validation_data=valid_ds, epochs=10, class_weight=class_weight_dict, callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBM1P8DKRknc",
    "outputId": "4355927b-7ff4-452c-b2de-c94a6ce9ad86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-542-b05528ab5e8f>:2: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  tf.keras.models.save_model(model, os.path.join('models', '1_sentiment_analysis.h5'))\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(model, os.path.join('models', '1_sentiment_analysis.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7v3zkiRRl-e",
    "outputId": "874621d2-90e8-4f3e-9939-b1d8b6cb2d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 82ms/step - loss: 0.4586 - accuracy: 0.8712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4585507810115814, 0.8712121248245239]"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0CbXYxTRo-B",
    "outputId": "7ac53a3c-6b3a-44bc-ce39-e7c6c9199800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         169472    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               66048     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 368643 (1.41 MB)\n",
      "Trainable params: 368643 (1.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Section 9.6\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Code listing 9.7\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    # Adding an Embedding layer\n",
    "    # You will see the following error if you don't filter out all zero (empty) records from the dataset\n",
    "    # these records return a vector of all zeros which leads the LSTM layer to error out\n",
    "    # CUDNN_STATUS_BAD_PARAM\n",
    "    # in tensorflow/stream_executor/cuda/cuda_dnn.cc(1496):\n",
    "    # 'cudnnSetRNNDataDescriptor(\n",
    "    #     data_desc.get(), data_type,\n",
    "    #     layout,\n",
    "    #     max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill\n",
    "    # )'\n",
    "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=128, mask_zero=True, input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
    "    # Defining Dense layers\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Defining a dropout layer\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Yym0U8CRuYp",
    "outputId": "97401adc-b018-4875-bfcd-58d4959b4155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining data pipelines\n",
      "\tDone...\n",
      "Using metric=val_loss and mode=min for EarlyStopping\n",
      "Epoch 1/10\n",
      "14/14 [==============================] - 7s 455ms/step - loss: 1.0926 - accuracy: 0.4312 - val_loss: 1.0695 - val_accuracy: 0.5271 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 3s 193ms/step - loss: 0.8624 - accuracy: 0.5306 - val_loss: 0.6904 - val_accuracy: 0.6434 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 3s 205ms/step - loss: 0.5882 - accuracy: 0.6258 - val_loss: 0.5661 - val_accuracy: 0.7442 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 3s 204ms/step - loss: 0.3694 - accuracy: 0.8496 - val_loss: 0.4761 - val_accuracy: 0.7752 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 4s 253ms/step - loss: 0.2217 - accuracy: 0.9248 - val_loss: 0.4454 - val_accuracy: 0.8217 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 3s 195ms/step - loss: 0.1188 - accuracy: 0.9612 - val_loss: 0.5812 - val_accuracy: 0.7984 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 3s 207ms/step - loss: 0.0540 - accuracy: 0.9812 - val_loss: 0.6040 - val_accuracy: 0.8450 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 4s 318ms/step - loss: 0.0330 - accuracy: 0.9933 - val_loss: 0.6028 - val_accuracy: 0.8605 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 3s 208ms/step - loss: 0.0082 - accuracy: 0.9994 - val_loss: 0.6901 - val_accuracy: 0.8527 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 3s 200ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.7463 - val_accuracy: 0.8527 - lr: 1.0000e-04\n",
      "It took 50.158605098724365 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "# Section 9.7\n",
    "\n",
    "print(\"Defining data pipelines\")\n",
    "batch_size = 128\n",
    "train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size,)\n",
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)\n",
    "print('\\tDone...')\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','2_sentiment_analysis_embeddings.log'))\n",
    "\n",
    "monitor_metric = 'val_loss'\n",
    "mode = 'min' if 'loss' in monitor_metric else 'max'\n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, validation_data=valid_ds, epochs=10, class_weight=class_weight_dict, callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4ho8s9JYkeK",
    "outputId": "ca5e658b-752c-45e9-e7ea-4d3997a1a707"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-548-915f9ed9bf8e>:2: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  tf.keras.models.save_model(model, os.path.join('models', '2_sentiment_analysis_embeddings.h5'))\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(model, os.path.join('models', '2_sentiment_analysis_embeddings.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXxPq6CQYnmK",
    "outputId": "dabaf74f-554f-4c32-fa30-664fa91a58be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4394 - accuracy: 0.8939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43941354751586914, 0.8939393758773804]"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLKtZYXQYsBb",
    "outputId": "674b957f-9182-4c7d-c7ed-22d8d8aaa9ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 98\n",
      "Pred: (98, 1)\n",
      "Y: (98,)\n"
     ]
    }
   ],
   "source": [
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\n",
    "\n",
    "# Go through the test data and gather all examples\n",
    "test_x = []\n",
    "test_pred = []\n",
    "test_y = []\n",
    "for x, y in test_ds:\n",
    "    test_x.append(x)\n",
    "    test_pred.append(model.predict(x, verbose=0))\n",
    "    test_y.append(y)\n",
    "\n",
    "# Check the sizes\n",
    "test_x = [doc for t in test_x for doc in t.numpy().tolist()]\n",
    "print(\"X: {}\".format(len(test_x)))\n",
    "test_pred = tf.concat(test_pred, axis=0).numpy()\n",
    "print(\"Pred: {}\".format(test_pred.shape))\n",
    "test_y = tf.concat(test_y, axis=0).numpy()\n",
    "print(\"Y: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSLgY0b5Yw7C",
    "outputId": "3b70c4fa-918a-4aa7-84c1-318beb660fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative reviews\n",
      "\n",
      "==================================================\n",
      "climate change may adverse effect business customer external supplier may adversely affect physical risk climate change include increase temperature sea level frequency severity adverse climatic event include fire storm flood drought effect whether acute chronic nature may directly impact us customer reputational damage environmental factor insurance risk business disruption \n",
      "\n",
      "climate change challenge face entire p c insurance industry particular home insurance business affect due change climate pattern increase number cost claim associate severe storm water damage make half home insurance claim unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk \n",
      "\n",
      "low carbon fuel standard exist propose environmental legislation regulation develop certain u.s. state canadian unk canadian federal government member european unk regulate carbon fuel standard could result increased cost reduce revenue potential regulation may negatively affect marketing unk â€™ unk unk oil unk product may require us purchase emission \n",
      "\n",
      "risk assessment rise average annual temperature could lead higher unk cost business customer unk temperature change could lead unk failure unk unk unk system storm could lead higher maintenance cost flood unk coastal could lead direct damage property unk affect customer â€™ business continuity unk unk unk unk unk \n",
      "\n",
      "failure comply environmental regulation may result unk fine penalty environmental protection order cost comply environmental regulation future may material adverse effect financial condition result operation cash flow unk environmental regulation could adverse impact unk â€™ reputation also risk unk could face litigation unk third party relate climate change environmental \n",
      "\n",
      "\n",
      "Most positive reviews\n",
      "\n",
      "==================================================\n",
      "unk local government coastal unk climate adaptation fund 4 million per unk three year establish unk local unk development coastal unk adaptation strategy coastal adaptation pilot project addition 3 million unk provide development implementation unk climate adaptation strategy collaboration local unk key stakeholder group 1 million per unk three \n",
      "\n",
      "unk unk commit create sustainable future employee customer community firm commit facilitate 200 billion financing 2020 support objective unk nation â€™ sustainable development goal focus address climate change advance social economic development unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk \n",
      "\n",
      "fund update guideline 1.5 billion sustainable investment program unk define sustainable invest fund unk criterion include unk manager strategy identify unk trend unk climate environment human right social inclusion economic development unk investment hold investment criterion fund â€™ investment unk unk unk unk unk unk unk unk unk unk \n",
      "\n",
      "mobilize private institutional capital mobilize capital support environmental social issue include transition low carbon economy example offer 100 sustainable unk mandate asset allocation fund base new dedicate unk strategic asset allocation private client global unk management unk unk collaboration develop range new unk pool impact investment unk unk unk \n",
      "\n",
      "2019 â€¢ provide unk million green loan unk group fund unk solar unk system unk unk site range commercial industrial government unk unk unk solar power system generate enough energy help reduce greenhouse gas emission unk tonne per year â€¢ unk unk asia â€™ first solar industry unk connect \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_pred = np.argsort(test_pred.flatten())\n",
    "min_pred = sorted_pred[:5]\n",
    "max_pred = sorted_pred[-5:]\n",
    "\n",
    "print(\"Most negative reviews\\n\")\n",
    "print(\"=\"*50)\n",
    "for i in min_pred:\n",
    "    print(\" \".join(tokenizer.sequences_to_texts([test_x[i]])), '\\n')\n",
    "\n",
    "print(\"\\nMost positive reviews\\n\")\n",
    "print(\"=\"*50)\n",
    "for i in max_pred:\n",
    "    print(\" \".join(tokenizer.sequences_to_texts([test_x[i]])), '\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
