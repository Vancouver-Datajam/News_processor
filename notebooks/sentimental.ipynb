{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VedVLW6y2vq5",
        "outputId": "f6ef7342-e472-459d-c214-b4a19e9edd0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.13.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import requests\n",
        "import zipfile\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.models as models\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from PIL import Image\n",
        "from PIL.PngImagePlugin import PngImageFile\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from functools import partial\n",
        "import nltk\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except:\n",
        "        print(\"Couldn't set memory_growth\")\n",
        "        pass\n",
        "\n",
        "\n",
        "def fix_random_seed(seed):\n",
        "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
        "    try:\n",
        "        np.random.seed(seed)\n",
        "    except NameError:\n",
        "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
        "    try:\n",
        "        tf.random.set_seed(seed)\n",
        "    except NameError:\n",
        "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
        "    try:\n",
        "        random.seed(seed)\n",
        "    except NameError:\n",
        "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
        "\n",
        "# Fixing the random seed\n",
        "random_seed=4321\n",
        "fix_random_seed(random_seed)\n",
        "\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "'''\n",
        "With regard to man-made climate change, the following scores reflect the\n",
        "sentiment of the tweet\n",
        "2 is factual\n",
        "1 is pro\n",
        "0 is neutral\n",
        "-1 is anti\n",
        "'''\n",
        "file_name = \"twitter_sentiment_data.csv\"\n",
        "zip_file_name = \"twitter.zip\"\n",
        "download_path = os.path.join('data', zip_file_name)\n",
        "\n",
        "if not os.path.exists('data'):\n",
        "        os.mkdir('data')\n",
        "\n",
        "if not os.path.exists(os.path.join('data', file_name)):\n",
        "  with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
        "    extraction_directory = \"data\"  # Define your extraction directory\n",
        "    os.makedirs(extraction_directory, exist_ok=True)\n",
        "    zip_ref.extractall(extraction_directory)\n",
        "    print(f\"ZIP file contents extracted to '{extraction_directory}'\")\n",
        "\n",
        "dataframe = pd.read_csv(os.path.join('data','twitter_sentiment_data.csv'))\n",
        "print(dataframe.head())\n",
        "print(dataframe.keys())"
      ],
      "metadata": {
        "id": "vVdc6SHbADxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2898c2-6d5d-4415-9aa5-51cb3d4927fc"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentiment                                            message  \\\n",
            "0         -1  @tiniebeany climate change is an interesting h...   \n",
            "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
            "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
            "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
            "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
            "\n",
            "              tweetid  \n",
            "0  792927353886371840  \n",
            "1  793124211518832641  \n",
            "2  793124402388832256  \n",
            "3  793124635873275904  \n",
            "4  793125156185137153  \n",
            "Index(['sentiment', 'message', 'tweetid'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Cleaning up data - removing null or empty data.\n",
        "Will need to change the column header depending on the dataset (\"message\")\n",
        "'''\n",
        "print(\"Before cleaning up: {}\".format(dataframe.shape))\n",
        "dataframe = dataframe[~dataframe[\"message\"].isna()]\n",
        "dataframe = dataframe[dataframe[\"message\"].str.strip().str.len()>0]\n",
        "print(\"After cleaning up: {}\".format(dataframe.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8-HV8C1HwBF",
        "outputId": "511a93a2-5f4b-4aba-9630-d0ec84cd3056"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before cleaning up: (43943, 3)\n",
            "After cleaning up: (43943, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of each sentiment in the dataset\n",
        "dataframe[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n02SBUiZIn8N",
        "outputId": "8e55eed5-33f6-416f-e712-00ea9c69ebe6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    22962\n",
              " 2     9276\n",
              " 0     7715\n",
              "-1     3990\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Depending on what we want, we can decide on how we want to map\n",
        "the sentiments to classses for training\n",
        "\n",
        "For now, we can work off the assumption of the following\n",
        "1 and 2 -> 1\n",
        "0 and -1 -> 0\n",
        "\n",
        "The final counts are printed out below\n",
        "'''\n",
        "dataframe[\"label\"] = dataframe[\"sentiment\"].map({1: 1, 2: 1, 0: 0, -1: 0})\n",
        "dataframe[\"label\"].value_counts()\n",
        "print(dataframe.head())\n",
        "print(\"\\n\")\n",
        "print( dataframe[\"label\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV6LhUQ0Jpq8",
        "outputId": "c0aad1c8-405f-439b-af98-cf4407b7f7fe"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentiment                                            message  \\\n",
            "0         -1  @tiniebeany climate change is an interesting h...   \n",
            "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
            "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
            "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
            "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
            "\n",
            "              tweetid  label  \n",
            "0  792927353886371840      0  \n",
            "1  793124211518832641      1  \n",
            "2  793124402388832256      1  \n",
            "3  793124635873275904      1  \n",
            "4  793125156185137153      1  \n",
            "\n",
            "\n",
            "1    32238\n",
            "0    11705\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle data\n",
        "dataframe = dataframe.sample(frac=1.0, random_state=random_seed)\n",
        "\n",
        "# split data into inputs and targets\n",
        "inputs, labels = dataframe[\"message\"], dataframe[\"label\"]"
      ],
      "metadata": {
        "id": "V_uxATKYK9ke"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Performing the following\n",
        "Lower case (nltk) - Turn \"I am\" to \"i am\"\n",
        "Remove numbers (regex) - Turn \"i am 24 years old\" to \"i am years old\"\n",
        "Remove stop words (nltk) - Turn \"i go to the shop\" to \"i go shop\"\n",
        "Lemmatize (nltk) - Turn \"i went to buy flowers\" to \"i go to buy flower\"\n",
        "'''\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger', download_dir='nltk')\n",
        "nltk.download('wordnet', download_dir='nltk')\n",
        "nltk.download('stopwords', download_dir='nltk')\n",
        "nltk.download('punkt', download_dir='nltk')\n",
        "nltk.download('omw-1.4', download_dir='nltk')\n",
        "nltk.data.path.append(os.path.abspath('nltk'))\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import string\n",
        "\n",
        "rerun = False\n",
        "\n",
        "# Define a lemmatizer (converts words to base form)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define the English stopwords\n",
        "EN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}\n",
        "\n",
        "# Code listing 9.2\n",
        "def clean_text(doc):\n",
        "    \"\"\" A function that cleans a given document (i.e. a text string)\"\"\"\n",
        "\n",
        "    # Turn to lower case\n",
        "    doc = doc.lower()\n",
        "    # the shortened form n't is expanded to not\n",
        "    doc = re.sub(pattern=r\"\\w+n\\'t \", repl=\"not \", string=doc)\n",
        "    # shortened forms like 'll 're 'd 've are removed as they don't add much value to this task\n",
        "    doc = re.sub(r\"(?:\\'ll |\\'re |\\'d |\\'ve )\", \" \", doc)\n",
        "    # numbers are removed\n",
        "    doc = re.sub(r\"/d+\",\"\", doc)\n",
        "    # break the text in to tokens (or words), while doing that ignore stopwords from the result\n",
        "    # stopwords again do not add any value to the task\n",
        "    tokens = [w for w in word_tokenize(doc) if w not in EN_STOPWORDS and w not in string.punctuation]\n",
        "\n",
        "    # Here we lemmatize the words in the tokens\n",
        "    # to lemmatize, we get the pos tag of each token and\n",
        "    # if it is N (noun) or V (verb) we lemmatize, else\n",
        "    # keep the original form\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    clean_text = [\n",
        "        lemmatizer.lemmatize(w, pos=p[0].lower()) \\\n",
        "        if p[0]=='N' or p[0]=='V' else w \\\n",
        "        for (w, p) in pos_tags\n",
        "    ]\n",
        "\n",
        "    # return the clean text\n",
        "    return clean_text\n",
        "\n",
        "# Run a sample\n",
        "sample_doc = 'She sells seashells by the seashore.'\n",
        "print(\"Before clean: {}\".format(sample_doc))\n",
        "print(\"After clean: {}\".format(clean_text(sample_doc)))\n",
        "\n",
        "if rerun or \\\n",
        "    not os.path.exists('sentiment_inputs.pkl') or \\\n",
        "    not os.path.exists('sentiment_labels.pkl'):\n",
        "    # Apply the transformation to the full text\n",
        "    # this is time consuming\n",
        "    print(\"\\nProcessing all the review data ... This can take some time (several minutes)\")\n",
        "    inputs = inputs.apply(lambda x: clean_text(x))\n",
        "    print(\"\\tDone\")\n",
        "\n",
        "    print(\"Saving the data\")\n",
        "    inputs.to_pickle('sentiment_inputs.pkl')\n",
        "    labels.to_pickle('sentiment_labels.pkl')\n",
        "\n",
        "else:\n",
        "    # Load the data from the disk\n",
        "    print(\"Data already found. If you want to rerun anyway, set rerun=True\")\n",
        "    inputs = pd.read_pickle( 'sentiment_inputs.pkl')\n",
        "    labels = pd.read_pickle( 'sentiment_labels.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_IX35eTLme5",
        "outputId": "ab7b5eaa-25bb-4690-89d4-f4bd56ed9a1b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to nltk...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to nltk...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to nltk...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to nltk...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to nltk...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before clean: She sells seashells by the seashore.\n",
            "After clean: ['sell', 'seashell', 'seashore']\n",
            "Data already found. If you want to rerun anyway, set rerun=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for actual, clean in zip(dataframe[\"message\"].iloc[:5], inputs.iloc[:5]):\n",
        "  print(\"Actual: {}\".format(actual))\n",
        "  print(\"Clean: {}\".format(clean))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juunXIRjMb2G",
        "outputId": "2feb7917-5c93-4b1e-df1a-a2678f9453a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual: RT @emptywheel: Area law man who doesn't believe in climate change (or much else science) worried CPD report isn't scientifically b…\n",
            "Clean: ['rt', 'emptywheel', 'area', 'law', 'man', 'not', 'believe', 'climate', 'change', 'much', 'else', 'science', 'worry', 'cpd', 'report', 'not', 'scientifically', 'b…']\n",
            "\n",
            "\n",
            "Actual: RT @SEIclimate: Think #climate change is a hoax? Visit #Norway, minister says | @ClimateHome https://t.co/qs4ZeTKnVa “we are seeing… \n",
            "Clean: ['rt', 'seiclimate', 'think', 'climate', 'change', 'hoax', 'visit', 'norway', 'minister', 'say', 'climatehome', 'http', '//t.co/qs4zetknva', '“', 'seeing…']\n",
            "\n",
            "\n",
            "Actual: I’ll build a man – we need global warming! I’ve said if Ivanka weren’t my office and you all know it! Please don't feel so\n",
            "Clean: ['’', 'build', 'man', '–', 'need', 'global', 'warming', '’', 'say', 'ivanka', '’', 'office', 'know', 'please', 'not', 'feel']\n",
            "\n",
            "\n",
            "Actual: RT @Trillburne: This is a rejection of basic, observable reality. Birtherism or global warming denial for professors who read The Atlanticâ€¦\n",
            "Clean: ['rt', 'trillburne', 'rejection', 'basic', 'observable', 'reality', 'birtherism', 'global', 'warm', 'denial', 'professor', 'read', 'atlanticâ€¦']\n",
            "\n",
            "\n",
            "Actual: Kenya’s Climate Change Bill Aims to Promote Low Carbon Growth http://t.co/yiWqOfn3ki\n",
            "Clean: ['kenya', '’', 'climate', 'change', 'bill', 'aim', 'promote', 'low', 'carbon', 'growth', 'http', '//t.co/yiwqofn3ki']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tweets and their labels\n",
        "print(inputs.head(n=20))\n",
        "print(labels.head(n=20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xamsFC_uNXdq",
        "outputId": "f856035d-e5a2-4808-c6a1-8fb1149c492e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13790    [rt, emptywheel, area, law, man, not, believe,...\n",
            "13123    [rt, seiclimate, think, climate, change, hoax,...\n",
            "19597    [’, build, man, –, need, global, warming, ’, s...\n",
            "33175    [rt, trillburne, rejection, basic, observable,...\n",
            "37670    [kenya, ’, climate, change, bill, aim, promote...\n",
            "14134    [rt, worldfnature, use, technology, fight, cli...\n",
            "25858    [must, read, doomed, climate, change, please, ...\n",
            "3823     [ã¢â‚¬å, “, carbon, capture, use, amp, storage...\n",
            "17529    [phoenix, face, reckon, climate, change, --, g...\n",
            "17449    [rt, climatehawk1, climate, change, nytimes, h...\n",
            "10991    [rt, bougiela, head, veteran, affair, 's, not,...\n",
            "11554    ['m, genuinely, concerned, let, man, office, n...\n",
            "13525    [rt, independent, trump, 's, new, executive, o...\n",
            "41059    [rt, climatereality, talk, current, event, el,...\n",
            "10592    [wonder, effect, climate, change, fly, flight,...\n",
            "39687    [rt, climatetreaty, imf, warn, affordable, ene...\n",
            "16550    [nobel, laureate, smash, global, warm, hoax, h...\n",
            "34818    [rt, shattenstone, donald, trump, climate, cha...\n",
            "34923    [rt, nit_set, not, rosy, economicsurvey18, mak...\n",
            "4445     [rt, ultravlolence, interviewer, 'what, think,...\n",
            "Name: message, dtype: object\n",
            "13790    1\n",
            "13123    1\n",
            "19597    0\n",
            "33175    1\n",
            "37670    1\n",
            "14134    1\n",
            "25858    1\n",
            "3823     1\n",
            "17529    1\n",
            "17449    1\n",
            "10991    1\n",
            "11554    1\n",
            "13525    1\n",
            "41059    1\n",
            "10592    0\n",
            "39687    1\n",
            "16550    0\n",
            "34818    0\n",
            "34923    1\n",
            "4445     0\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_valid_test_split(inputs, labels, train_fraction=0.8):\n",
        "    \"\"\" Splits a given dataset into three sets; training, validation and test \"\"\"\n",
        "\n",
        "    # Separate indices of negative and positive data points\n",
        "    neg_indices = pd.Series(labels.loc[(labels==0)].index)\n",
        "    pos_indices = pd.Series(labels.loc[(labels==1)].index)\n",
        "\n",
        "    n_valid = int(min([len(neg_indices), len(pos_indices)]) * ((1-train_fraction)/2.0))\n",
        "    n_test = n_valid\n",
        "\n",
        "    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)\n",
        "    neg_valid_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds)].sample(n=n_test, random_state=random_seed)\n",
        "    neg_train_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds.tolist()+neg_valid_inds.tolist())]\n",
        "\n",
        "    pos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed)\n",
        "    pos_valid_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds)].sample(n=n_test, random_state=random_seed)\n",
        "    pos_train_inds = pos_indices.loc[\n",
        "        ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())\n",
        "    ]\n",
        "\n",
        "    tr_x = inputs.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "    tr_y = labels.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "    v_x = inputs.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "    v_y = labels.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "    ts_x = inputs.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "    ts_y = labels.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
        "\n",
        "    print('Training data: {}'.format(len(tr_x)))\n",
        "    print('Validation data: {}'.format(len(v_x)))\n",
        "    print('Test data: {}'.format(len(ts_x)))\n",
        "\n",
        "    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)\n",
        "\n",
        "(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(inputs, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvVSvgq0Nfu8",
        "outputId": "c32e1748-69c9-4363-9a74-f51a32601774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data: 39263\n",
            "Validation data: 2340\n",
            "Test data: 2340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Some sample targets\")\n",
        "print(tr_y.head(n=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l198HxhNjI_",
        "outputId": "dad4469d-b31e-49ef-ea18-fbe27799428f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some sample targets\n",
            "42061    0\n",
            "37226    1\n",
            "36339    1\n",
            "42707    1\n",
            "14364    1\n",
            "11999    1\n",
            "8560     1\n",
            "3910     1\n",
            "24367    1\n",
            "37011    1\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Analysis of vocabulary\n",
        "'''\n",
        "from collections import Counter\n",
        "# Create a large list which contains all the words in all the reviews\n",
        "data_list = [w for doc in tr_x for w in doc]\n",
        "\n",
        "# Create a Counter object from that list\n",
        "# Counter returns a dictionary, where key is a word and the value is the frequency\n",
        "cnt = Counter(data_list)\n",
        "\n",
        "# Convert the result to a pd.Series\n",
        "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
        "# Print most common words\n",
        "print(freq_df.head(n=10))\n",
        "\n",
        "# Print summary statistics\n",
        "print(freq_df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rji1wdyRNpCW",
        "outputId": "d4a0b9db-14b7-43fc-8c04-9ab79877712c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "climate    31286\n",
            "change     30459\n",
            "http       23220\n",
            "rt         22739\n",
            "global      9222\n",
            "not         6189\n",
            "'s          5656\n",
            "warming     4979\n",
            "trump       4514\n",
            "warm        3754\n",
            "dtype: int64\n",
            "count    71544.000000\n",
            "mean         7.382226\n",
            "std        215.082333\n",
            "min          1.000000\n",
            "25%          1.000000\n",
            "50%          1.000000\n",
            "75%          1.000000\n",
            "max      31286.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Analysing thesequence length (number of words) of tweets\n",
        "'''\n",
        "# Create a pd.Series, which contain the sequence length for each review\n",
        "seq_length_ser = tr_x.str.len()\n",
        "\n",
        "# Get the median as well as summary statistics of the sequence length\n",
        "print(\"\\nSome summary statistics\")\n",
        "print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
        "seq_length_ser.describe()\n",
        "\n",
        "print(\"\\nComputing the statistics between the 10% and 90% quantiles (to ignore outliers)\")\n",
        "p_10 = seq_length_ser.quantile(0.1)\n",
        "p_90 = seq_length_ser.quantile(0.9)\n",
        "\n",
        "seq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe(percentiles=[0.33, 0.66])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyJu7jMDNxCn",
        "outputId": "df466c16-400f-4040-9715-4e282d13bd2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Some summary statistics\n",
            "Median length: 14.0\n",
            "\n",
            "\n",
            "Computing the statistics between the 10% and 90% quantiles (to ignore outliers)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    30845.000000\n",
              "mean        13.282639\n",
              "std          1.956487\n",
              "min          9.000000\n",
              "33%         13.000000\n",
              "50%         14.000000\n",
              "66%         14.000000\n",
              "max         16.000000\n",
              "Name: message, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocab = (freq_df >= 14).sum()\n",
        "print(\"Using a vocabulary of size: {}\".format(n_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ5M0xZPN-w2",
        "outputId": "85c4d24b-a3fa-4c09-a9b3-9e886fbf770f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a vocabulary of size: 3169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Transforming text to numbers\n",
        "'''\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Define a tokenizer that will convert words to IDs\n",
        "# words that are less frequent will be replaced by 'unk'\n",
        "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
        "\n",
        "# Fit the tokenizer on the data\n",
        "tokenizer.fit_on_texts(tr_x.tolist())\n",
        "\n",
        "# Convert all of train/validation/test data to sequences of IDs\n",
        "tr_x = tokenizer.texts_to_sequences(tr_x.tolist())\n",
        "v_x = tokenizer.texts_to_sequences(v_x.tolist())\n",
        "ts_x = tokenizer.texts_to_sequences(ts_x.tolist())"
      ],
      "metadata": {
        "id": "2d-bVWLTPH0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the attributes of the tokenizer\n",
        "word = \"rt\"\n",
        "wid = tokenizer.word_index[word]\n",
        "print(\"The word id for \\\"{}\\\" is: {}\".format(word, wid))\n",
        "wid = 4\n",
        "word = tokenizer.index_word[wid]\n",
        "print(\"The word for id {} is: {}\".format(wid, word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hxnRkmhPXvH",
        "outputId": "b598ff94-c7fa-46ee-9448-ad0ba6c9c2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word id for \"rt\" is: 5\n",
            "The word for id 4 is: http\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert words to IDs\n",
        "\n",
        "# Vocabs not used get assigned id of 1 I think, so since they're not being\n",
        "# used in the training, it doesn't actually matter\n",
        "test_text = [\n",
        "    ['rt', 'emptywheel', 'area', 'law', 'man', 'not', 'believe', 'climate', 'change', 'much', 'else', 'science', 'worry', 'cpd', 'report', 'not', 'scientifically', 'b…'],\n",
        "    ['rt', 'seiclimate', 'think', 'climate', 'change', 'hoax', 'visit', 'norway', 'minister', 'say', 'climatehome', 'http', '//t.co/qs4zetknva', '“', 'seeing…'],\n",
        "    ['’', 'build', 'man', '–', 'need', 'global', 'warming', '’', 'say', 'ivanka', '’', 'office', 'know', 'please', 'not', 'feel'],\n",
        "    ['rt', 'trillburne', 'rejection', 'basic', 'observable', 'reality', 'birtherism', 'global', 'warm', 'denial', 'professor', 'read', 'atlanticâ€¦'],\n",
        "    ['kenya', '’', 'climate', 'change', 'bill', 'aim', 'promote', 'low', 'carbon', 'growth', 'http', '//t.co/yiwqofn3ki'],\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_text)\n",
        "\n",
        "for text, seq in zip(test_text, test_seq):\n",
        "    print(\"Text: {}\".format(text))\n",
        "    print(\"Sequence: {}\".format(seq))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5W_YNYiPdTx",
        "outputId": "612eb7db-bbde-4ace-8568-129fcf41d477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: ['rt', 'emptywheel', 'area', 'law', 'man', 'not', 'believe', 'climate', 'change', 'much', 'else', 'science', 'worry', 'cpd', 'report', 'not', 'scientifically', 'b…']\n",
            "Sequence: [5, 1, 843, 607, 143, 7, 15, 2, 3, 134, 818, 36, 230, 1, 106, 7, 1, 1603]\n",
            "\n",
            "\n",
            "Text: ['rt', 'seiclimate', 'think', 'climate', 'change', 'hoax', 'visit', 'norway', 'minister', 'say', 'climatehome', 'http', '//t.co/qs4zetknva', '“', 'seeing…']\n",
            "Sequence: [5, 1, 24, 2, 3, 48, 789, 2786, 653, 14, 1726, 4, 1, 70, 1]\n",
            "\n",
            "\n",
            "Text: ['’', 'build', 'man', '–', 'need', 'global', 'warming', '’', 'say', 'ivanka', '’', 'office', 'know', 'please', 'not', 'feel']\n",
            "Sequence: [16, 413, 143, 216, 35, 6, 9, 16, 14, 1389, 16, 570, 39, 226, 7, 423]\n",
            "\n",
            "\n",
            "Text: ['rt', 'trillburne', 'rejection', 'basic', 'observable', 'reality', 'birtherism', 'global', 'warm', 'denial', 'professor', 'read', 'atlanticâ€¦']\n",
            "Sequence: [5, 1, 1, 896, 1, 309, 1, 6, 11, 159, 1607, 157, 1]\n",
            "\n",
            "\n",
            "Text: ['kenya', '’', 'climate', 'change', 'bill', 'aim', 'promote', 'low', 'carbon', 'growth', 'http', '//t.co/yiwqofn3ki']\n",
            "Sequence: [1613, 16, 2, 3, 284, 893, 1043, 1388, 87, 944, 4, 1]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tf_pipeline(text_seq, labels, batch_size=64, bucket_boundaries=[5,15], max_length=50, shuffle=False):\n",
        "    \"\"\" Define a data pipeline that converts sequences to batches of data \"\"\"\n",
        "\n",
        "    # Concatenate the label and the input sequence so that we don't mess up the order when we shuffle\n",
        "    data_seq = [[b]+a for a,b in zip(text_seq, labels) ]\n",
        "    # Define the variable sequence dataset as a ragged tensor\n",
        "    tf_data = tf.ragged.constant(data_seq)[:,:max_length]\n",
        "    # Create a dataset out of the ragged tensor\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(tf_data)\n",
        "\n",
        "    text_ds = text_ds.filter(lambda x: tf.size(x)>1)\n",
        "    # Bucketing the data\n",
        "    # Bucketing assign each sequence to a bucket depending on the length\n",
        "    # If you define bucket boundaries as [5, 15], then you get buckets,\n",
        "    # [0, 5], [5, 15], [15,inf]\n",
        "    bucket_fn = tf.data.experimental.bucket_by_sequence_length(\n",
        "        lambda x: tf.cast(tf.shape(x)[0],'int32'),\n",
        "        bucket_boundaries=bucket_boundaries,\n",
        "        bucket_batch_sizes=[batch_size,batch_size,batch_size],\n",
        "        padded_shapes=None,\n",
        "        padding_values=0,\n",
        "        pad_to_bucket_boundary=False\n",
        "    )\n",
        "\n",
        "    # Apply bucketing\n",
        "    text_ds = text_ds.map(lambda x: x).apply(bucket_fn)\n",
        "\n",
        "    # Shuffle the data\n",
        "    if shuffle:\n",
        "        text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n",
        "\n",
        "    # Split the data to inputs and labels\n",
        "    text_ds = text_ds.map(lambda x: (x[:,1:], x[:,0]))\n",
        "\n",
        "    return text_ds"
      ],
      "metadata": {
        "id": "yIm7qNsWQnbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Testing the bucketing function '''\n",
        "train_ds = get_tf_pipeline(tr_x, tr_y, shuffle=True)\n",
        "valid_ds = get_tf_pipeline(v_x, v_y)\n",
        "\n",
        "print(\"Some training data ...\")\n",
        "for x,y in train_ds.take(2):\n",
        "    print(\"Input sequence shape: {}\".format(x.shape))\n",
        "    print(y)\n",
        "\n",
        "print(\"\\nSome validation data ...\")\n",
        "for x,y in valid_ds.take(2):\n",
        "    print(\"Input sequence shape: {}\".format(x.shape))\n",
        "    print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu6p8xJXQwnK",
        "outputId": "5912ffee-5404-4396-9f3f-9afc59e707c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-46-c7d6e5233fe0>:16: bucket_by_sequence_length (from tensorflow.python.data.experimental.ops.grouping) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.bucket_by_sequence_length(...)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some training data ...\n",
            "Input sequence shape: (64, 20)\n",
            "tf.Tensor(\n",
            "[1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1], shape=(64,), dtype=int32)\n",
            "Input sequence shape: (64, 21)\n",
            "tf.Tensor(\n",
            "[1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0], shape=(64,), dtype=int32)\n",
            "\n",
            "Some validation data ...\n",
            "Input sequence shape: (64, 22)\n",
            "tf.Tensor(\n",
            "[0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0], shape=(64,), dtype=int32)\n",
            "Input sequence shape: (64, 13)\n",
            "tf.Tensor(\n",
            "[0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0], shape=(64,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "class OnehotEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, depth, **kwargs):\n",
        "        super(OnehotEncoder, self).__init__(**kwargs)\n",
        "        self.depth = depth\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        inputs = tf.cast(inputs, 'int32')\n",
        "\n",
        "        if len(inputs.shape) == 3:\n",
        "            inputs = inputs[:,:,0]\n",
        "\n",
        "        return tf.one_hot(inputs, depth=self.depth)\n",
        "\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({'depth': self.depth})\n",
        "        return config\n",
        "\n",
        "# You will see the following error if you don't filter out all zero (empty) records from the dataset\n",
        "# these records return a vector of all zeros which leads the LSTM layer to error out\n",
        "# CUDNN_STATUS_BAD_PARAM\n",
        "# in tensorflow/stream_executor/cuda/cuda_dnn.cc(1496):\n",
        "# 'cudnnSetRNNDataDescriptor(\n",
        "#     data_desc.get(), data_type,\n",
        "#     layout,\n",
        "#     max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill\n",
        "# )'\n",
        "\n",
        "# Code listing 9.5\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=(None,)),\n",
        "    # Create a mask to mask out zero inputs\n",
        "    tf.keras.layers.Masking(mask_value=0),\n",
        "    # After creating the mask, convert inputs to onehot encoded inputs\n",
        "    OnehotEncoder(depth=n_vocab),\n",
        "    # Defining an LSTM layer\n",
        "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
        "    # Defining a Dense layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyhiKfDhQ7DG",
        "outputId": "df2a3a50-8a7b-41ef-dfea-0e926e4ad056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda (Lambda)             (None, None, 1)           0         \n",
            "                                                                 \n",
            " masking (Masking)           (None, None, 1)           0         \n",
            "                                                                 \n",
            " onehot_encoder (OnehotEnco  (None, None, 3169)        0         \n",
            " der)                                                            \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               1688576   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               66048     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1755137 (6.70 MB)\n",
            "Trainable params: 1755137 (6.70 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Checking the mask '''\n",
        "inp = tf.expand_dims(tf.constant([[2,3,4,0,0], [2,4,6,12,0]], dtype='int32'),axis=-1)\n",
        "# Create a mask to mask out zero inputs\n",
        "mask_out = tf.keras.layers.Masking(mask_value=0)(inp)\n",
        "print(\"Masking layer's mask\")\n",
        "print(mask_out._keras_mask)\n",
        "# After creating the mask, convert inputs to onehot encoded inputs\n",
        "onehot_out = OnehotEncoder(depth=10)(mask_out)\n",
        "print(\"Onehot encoder layer's mask\")\n",
        "print(onehot_out._keras_mask)\n",
        "# Defining an LSTM layer\n",
        "lstm_out = tf.keras.layers.LSTM(24, return_state=False, return_sequences=False)(\n",
        "    onehot_out, mask=onehot_out._keras_mask\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4JMMAf_RE40",
        "outputId": "14555fcb-d27a-499d-e33e-09638ca90d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masking layer's mask\n",
            "tf.Tensor(\n",
            "[[ True  True  True False False]\n",
            " [ True  True  True  True False]], shape=(2, 5), dtype=bool)\n",
            "Onehot encoder layer's mask\n",
            "tf.Tensor(\n",
            "[[ True  True  True False False]\n",
            " [ True  True  True  True False]], shape=(2, 5), dtype=bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining data pipelines\")\n",
        "\n",
        "# Using a batch size of 128\n",
        "batch_size = 128\n",
        "\n",
        "train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)\n",
        "valid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size)\n",
        "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)\n",
        "print('\\tDone...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZdk8WG8RMiy",
        "outputId": "2b452af7-41b7-487e-ab91-3025117ec23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining data pipelines\n",
            "\tDone...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# There is a class imbalance in the data therefore we are defining a weight for negative inputs\n",
        "neg_weight = (tr_y==1).sum()/(tr_y==0).sum()\n",
        "print(\"Will be using a weight of {} for negative samples\".format(neg_weight))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm_3Jv-xRRpZ",
        "outputId": "45909369-e9fd-4d7c-8342-45f9f8222e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will be using a weight of 3.19252536038441 for negative samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 9.5\n",
        "\n",
        "os.makedirs('eval', exist_ok=True)\n",
        "\n",
        "# Logging the performance metrics to a CSV file\n",
        "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','1_sentiment_analysis.log'))\n",
        "\n",
        "monitor_metric = 'val_loss'\n",
        "mode = 'min'\n",
        "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
        "\n",
        "# Reduce LR callback\n",
        "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8\n",
        ")\n",
        "\n",
        "# EarlyStopping itself increases the memory requirement\n",
        "# restore_best_weights will increase the memory req for large models\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "t1 = time.time()\n",
        "\n",
        "model.fit(train_ds, validation_data=valid_ds, epochs=10, class_weight={0:neg_weight, 1:1.0}, callbacks=[es_callback, lr_callback, csv_logger])\n",
        "t2 = time.time()\n",
        "\n",
        "print(\"It took {} seconds to complete the training\".format(t2-t1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCIS-PApRVAj",
        "outputId": "4de1f1bd-3897-4f7f-ebdc-a4b6fa9d9f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using metric=val_loss and mode=min for EarlyStopping\n",
            "Epoch 1/10\n",
            "308/308 [==============================] - 150s 464ms/step - loss: 0.7473 - accuracy: 0.7603 - val_loss: 0.4565 - val_accuracy: 0.7970 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "308/308 [==============================] - 127s 404ms/step - loss: 0.5876 - accuracy: 0.8193 - val_loss: 0.4266 - val_accuracy: 0.7991 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "308/308 [==============================] - 126s 399ms/step - loss: 0.5092 - accuracy: 0.8412 - val_loss: 0.4618 - val_accuracy: 0.7966 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "308/308 [==============================] - 124s 395ms/step - loss: 0.4504 - accuracy: 0.8544 - val_loss: 0.4884 - val_accuracy: 0.7838 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "308/308 [==============================] - 121s 386ms/step - loss: 0.3887 - accuracy: 0.8705 - val_loss: 0.5940 - val_accuracy: 0.7731 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "308/308 [==============================] - 125s 400ms/step - loss: 0.2891 - accuracy: 0.9020 - val_loss: 0.7893 - val_accuracy: 0.7701 - lr: 1.0000e-04\n",
            "Epoch 7/10\n",
            "308/308 [==============================] - 121s 385ms/step - loss: 0.2660 - accuracy: 0.9094 - val_loss: 0.8941 - val_accuracy: 0.7658 - lr: 1.0000e-04\n",
            "Epoch 8/10\n",
            "308/308 [==============================] - 119s 379ms/step - loss: 0.2500 - accuracy: 0.9142 - val_loss: 1.0038 - val_accuracy: 0.7692 - lr: 1.0000e-04\n",
            "It took 1141.5238058567047 seconds to complete the training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('models', exist_ok=True)\n",
        "tf.keras.models.save_model(model, os.path.join('models', '1_sentiment_analysis.h5'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBM1P8DKRknc",
        "outputId": "619c6711-f198-48e0-acb7-346571c63e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-b05528ab5e8f>:2: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  tf.keras.models.save_model(model, os.path.join('models', '1_sentiment_analysis.h5'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7v3zkiRRl-e",
        "outputId": "39231521-779a-40d5-c97f-0e6fc808512c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 8s 373ms/step - loss: 1.0420 - accuracy: 0.7791\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0420219898223877, 0.7790598273277283]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 9.6\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "# Code listing 9.7\n",
        "model = tf.keras.models.Sequential([\n",
        "\n",
        "    # Adding an Embedding layer\n",
        "    # You will see the following error if you don't filter out all zero (empty) records from the dataset\n",
        "    # these records return a vector of all zeros which leads the LSTM layer to error out\n",
        "    # CUDNN_STATUS_BAD_PARAM\n",
        "    # in tensorflow/stream_executor/cuda/cuda_dnn.cc(1496):\n",
        "    # 'cudnnSetRNNDataDescriptor(\n",
        "    #     data_desc.get(), data_type,\n",
        "    #     layout,\n",
        "    #     max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill\n",
        "    # )'\n",
        "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=128, mask_zero=True, input_shape=(None,)),\n",
        "    # Defining an LSTM layer\n",
        "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
        "    # Defining Dense layers\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Defining a dropout layer\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0CbXYxTRo-B",
        "outputId": "0c98ddb9-f68d-4dc7-cd18-c7a28354c34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 128)         405760    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               66048     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 603905 (2.30 MB)\n",
            "Trainable params: 603905 (2.30 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 9.7\n",
        "\n",
        "print(\"Defining data pipelines\")\n",
        "batch_size = 128\n",
        "train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)\n",
        "valid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size,)\n",
        "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)\n",
        "print('\\tDone...')\n",
        "\n",
        "os.makedirs('eval', exist_ok=True)\n",
        "\n",
        "# Logging the performance metrics to a CSV file\n",
        "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','2_sentiment_analysis_embeddings.log'))\n",
        "\n",
        "monitor_metric = 'val_loss'\n",
        "mode = 'min' if 'loss' in monitor_metric else 'max'\n",
        "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
        "\n",
        "# Reduce LR callback\n",
        "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8\n",
        ")\n",
        "\n",
        "# EarlyStopping itself increases the memory requirement\n",
        "# restore_best_weights will increase the memory req for large models\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False\n",
        ")\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "model.fit(train_ds, validation_data=valid_ds, epochs=10, class_weight={0:neg_weight, 1:1.0}, callbacks=[es_callback, lr_callback, csv_logger])\n",
        "t2 = time.time()\n",
        "\n",
        "print(\"It took {} seconds to complete the training\".format(t2-t1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yym0U8CRuYp",
        "outputId": "0177c17b-7eb1-468f-9d21-a81f6c01b571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining data pipelines\n",
            "\tDone...\n",
            "Using metric=val_loss and mode=min for EarlyStopping\n",
            "Epoch 1/10\n",
            "308/308 [==============================] - 42s 109ms/step - loss: 0.7422 - accuracy: 0.7747 - val_loss: 0.4478 - val_accuracy: 0.7859 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "308/308 [==============================] - 32s 98ms/step - loss: 0.6012 - accuracy: 0.8174 - val_loss: 0.4437 - val_accuracy: 0.7927 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "308/308 [==============================] - 32s 93ms/step - loss: 0.5323 - accuracy: 0.8324 - val_loss: 0.4591 - val_accuracy: 0.7927 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "308/308 [==============================] - 30s 91ms/step - loss: 0.4674 - accuracy: 0.8503 - val_loss: 0.5136 - val_accuracy: 0.7838 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "308/308 [==============================] - 31s 93ms/step - loss: 0.4258 - accuracy: 0.8561 - val_loss: 0.5923 - val_accuracy: 0.7782 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "308/308 [==============================] - 31s 93ms/step - loss: 0.3257 - accuracy: 0.8891 - val_loss: 0.7352 - val_accuracy: 0.7761 - lr: 1.0000e-04\n",
            "Epoch 7/10\n",
            "308/308 [==============================] - 30s 91ms/step - loss: 0.3006 - accuracy: 0.8960 - val_loss: 0.8331 - val_accuracy: 0.7786 - lr: 1.0000e-04\n",
            "Epoch 8/10\n",
            "308/308 [==============================] - 30s 90ms/step - loss: 0.2853 - accuracy: 0.9000 - val_loss: 0.8909 - val_accuracy: 0.7726 - lr: 1.0000e-04\n",
            "It took 278.5666387081146 seconds to complete the training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('models', exist_ok=True)\n",
        "tf.keras.models.save_model(model, os.path.join('models', '2_sentiment_analysis_embeddings.h5'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4ho8s9JYkeK",
        "outputId": "91da043d-faa1-4162-d271-1b975ff22a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-915f9ed9bf8e>:2: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  tf.keras.models.save_model(model, os.path.join('models', '2_sentiment_analysis_embeddings.h5'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\n",
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXxPq6CQYnmK",
        "outputId": "209bb729-7888-466e-fbf0-379a4cadd85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 3s 83ms/step - loss: 0.8756 - accuracy: 0.7791\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8755584955215454, 0.7790598273277283]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\n",
        "\n",
        "# Go through the test data and gather all examples\n",
        "test_x = []\n",
        "test_pred = []\n",
        "test_y = []\n",
        "for x, y in test_ds:\n",
        "    test_x.append(x)\n",
        "    test_pred.append(model.predict(x, verbose=0))\n",
        "    test_y.append(y)\n",
        "\n",
        "# Check the sizes\n",
        "test_x = [doc for t in test_x for doc in t.numpy().tolist()]\n",
        "print(\"X: {}\".format(len(test_x)))\n",
        "test_pred = tf.concat(test_pred, axis=0).numpy()\n",
        "print(\"Pred: {}\".format(test_pred.shape))\n",
        "test_y = tf.concat(test_y, axis=0).numpy()\n",
        "print(\"Y: {}\".format(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLKtZYXQYsBb",
        "outputId": "6738c5ec-76a2-4220-ebaa-0999d989492c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: 2340\n",
            "Pred: (2340, 1)\n",
            "Y: (2340,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_pred = np.argsort(test_pred.flatten())\n",
        "min_pred = sorted_pred[:5]\n",
        "max_pred = sorted_pred[-5:]\n",
        "\n",
        "print(\"Most negative reviews\\n\")\n",
        "print(\"=\"*50)\n",
        "for i in min_pred:\n",
        "    print(\" \".join(tokenizer.sequences_to_texts([test_x[i]])), '\\n')\n",
        "\n",
        "print(\"\\nMost positive reviews\\n\")\n",
        "print(\"=\"*50)\n",
        "for i in max_pred:\n",
        "    print(\" \".join(tokenizer.sequences_to_texts([test_x[i]])), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSLgY0b5Yw7C",
        "outputId": "ac9f4c1d-5126-449c-99dd-a7a9e3cee031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most negative reviews\n",
            "\n",
            "==================================================\n",
            "rt ultravlolence interviewer 'what think global warm melania trump 'hello ... 's kind 's strong 's great ... cã¢â‚¬â¦ unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk unk \n",
            "\n",
            "rt stevesgoddard 3 different civilization wipe climate change greenland past unk yr http unk htt… unk unk unk unk unk unk \n",
            "\n",
            "rt civiljustus expect us care global warm penguin penguin crime http //t.co/hypyswhvvv unk \n",
            "\n",
            "rt civiljustus expect us care global warm penguin penguin crime http //t.co/hypyswhvvv unk \n",
            "\n",
            "rt civiljustus expect us care global warm penguin penguin crime http //t.co/hypyswhvvv unk \n",
            "\n",
            "\n",
            "Most positive reviews\n",
            "\n",
            "==================================================\n",
            "rt stephenschlegel 's thinking 's go die husband not believe climate change http //t.co/sjofonã¢â‚¬â¦ unk unk unk unk unk unk unk unk unk \n",
            "\n",
            "rt bentler http unk lloyd 's london divest coal climate change climate insurance invest http //t.co/â€¦ unk unk unk unk unk unk \n",
            "\n",
            "disaster planet climate change denier hand power global implication trump 's ignorance http unk unk unk unk unk unk unk unk unk unk \n",
            "\n",
            "rt sensanders not address climate change conflict limited resource must listen scientist not fossiã¢â‚¬â¦ unk unk unk unk unk unk unk unk unk \n",
            "\n",
            "rt eriksolheim unk parisagreement pledge third action need avoid worst effect climate change … unk unk unk unk unk unk unk unk \n",
            "\n"
          ]
        }
      ]
    }
  ]
}