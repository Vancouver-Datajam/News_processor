{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e72d4cff",
   "metadata": {},
   "source": [
    "# Reading Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2d39e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "                                             message  sentiment\n",
      "0  − Scope 3: Optional scope that includes indire...          1\n",
      "1  The Group is not aware of any noise pollution ...          0\n",
      "2  Global climate change could exacerbate certain...          0\n",
      "3  Setting an investment horizon is part and parc...          0\n",
      "4  Climate change the physical impacts of climate...          0\n",
      "Index(['message', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"climatebert/climate_sentiment\")\n",
    "\n",
    "# Convert the train split to a Pandas DataFrame\n",
    "train_df = dataset['train'].to_pandas()\n",
    "\n",
    "# Convert the test split to a Pandas DataFrame\n",
    "test_df = dataset['test'].to_pandas()\n",
    "\n",
    "# Concatenate train and test dataframes\n",
    "dataframe = pd.concat([train_df, test_df], ignore_index=True)\n",
    "dataframe.columns = ['message', 'sentiment']\n",
    "print(dataframe.head())\n",
    "print(dataframe.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ecac7",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6b75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_message(text):\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the \"message\" column\n",
    "dataframe['message'] = dataframe['message'].apply(clean_message)\n",
    "\n",
    "def tokenize_message(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "dataframe['tokenized_message'] = dataframe['message'].apply(tokenize_message)\n",
    "\n",
    "# Remove stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Change 'english' to your language if needed\n",
    "\n",
    "def remove_stopwords(tokenized_text):\n",
    "    return [word for word in tokenized_text if word.lower() not in stop_words]\n",
    "\n",
    "dataframe['tokenized_message'] = dataframe['tokenized_message'].apply(remove_stopwords)\n",
    "\n",
    "# Convert all words to lowercase\n",
    "def convert_to_lowercase(tokenized_text):\n",
    "    return [word.lower() for word in tokenized_text]\n",
    "\n",
    "dataframe['tokenized_message'] = dataframe['tokenized_message'].apply(convert_to_lowercase)\n",
    "\n",
    "dataframe = dataframe[['sentiment', 'tokenized_message']]\n",
    "\n",
    "X = dataframe['tokenized_message']\n",
    "y = dataframe['sentiment']\n",
    "X_train, X_test, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert lists of tokens back into strings\n",
    "X_train = X_train.apply(lambda x: ' '.join(x))\n",
    "X_test = X_test.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b50a6",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d5ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe['tokenized_message']\n",
    "y = dataframe['sentiment']\n",
    "X_train, X_test, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74a6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists of tokens back into strings\n",
    "X_train = X_train.apply(lambda x: ' '.join(x))\n",
    "X_test = X_test.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2881b",
   "metadata": {},
   "source": [
    "# Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090e4c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Accuracy: 0.8787878787878788\n",
      "Naive Bayes Validation Accuracy: 0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "# Train the model on the training data\n",
    "naive_bayes.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_nb = naive_bayes.predict(X_train_tfidf)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred_nb = naive_bayes.predict(X_valid_tfidf)\n",
    "\n",
    "# Calculate accuracy on the training and validation sets\n",
    "training_accuracy_nb = accuracy_score(y_train, y_train_pred_nb)\n",
    "validation_accuracy_nb = accuracy_score(y_valid, y_valid_pred_nb)\n",
    "\n",
    "print(\"Naive Bayes Training Accuracy:\", training_accuracy_nb)\n",
    "print(\"Naive Bayes Validation Accuracy:\", validation_accuracy_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6110eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Accuracy: 0.8787878787878788\n",
      "Naive Bayes Validation Accuracy: 0.7083333333333334\n",
      "Best Alpha: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "# Define a range of alpha values to try\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "    'fit_prior': [True, False],\n",
    "    'class_prior': [None, [0.3, 0.4, 0.3]]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(naive_bayes, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best hyperparameters from grid search\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_fit_prior = grid_search.best_params_['fit_prior']\n",
    "best_class_prior = grid_search.best_params_['class_prior']\n",
    "\n",
    "# Initialize a new Naive Bayes model with the best hyperparameters\n",
    "best_naive_bayes = MultinomialNB(\n",
    "    alpha=best_alpha,\n",
    "    fit_prior=best_fit_prior,\n",
    "    class_prior=best_class_prior\n",
    ")\n",
    "\n",
    "# Train the final model on the entire training dataset\n",
    "best_naive_bayes.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train the model on the training data\n",
    "naive_bayes.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred_nb = naive_bayes.predict(X_train_tfidf)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred_nb = naive_bayes.predict(X_valid_tfidf)\n",
    "\n",
    "# Calculate accuracy on the training and validation sets\n",
    "training_accuracy_nb = accuracy_score(y_train, y_train_pred_nb)\n",
    "validation_accuracy_nb = accuracy_score(y_valid, y_valid_pred_nb)\n",
    "\n",
    "print(\"Naive Bayes Training Accuracy:\", training_accuracy_nb)\n",
    "print(\"Naive Bayes Validation Accuracy:\", validation_accuracy_nb)\n",
    "print(\"Best Alpha:\", best_alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc6b2a",
   "metadata": {},
   "source": [
    "# Limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b712a12",
   "metadata": {},
   "source": [
    "Despite our best efforts, we encountered challenges with the Naive Bayes model. Even after extensive hyperparameter tuning, we faced overfitting issues, where the model performed exceptionally well on the training data but struggled to generalize to unseen validation data. Specifically, the Naive Bayes model achieved a high training accuracy of 87.88%, but its validation accuracy remained at a comparatively lower 70.83%. This suggests that the model may have become overly complex or that we need to explore further techniques to control overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
